{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theprashasst/Medical-Reasoning-DeepSeek-R1/blob/main/prototyping_Intern_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJ--FDljmD5O"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "%%capture\n",
        "\n",
        "!pip install unsloth # install unsloth\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # Also get the latest version Unsloth!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ugx77sRss5S"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "hugging_face_token=userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxDbF79x0DSO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain_community transformers fastapi uvicorn pyngrok gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQheZy7A3OKo"
      },
      "outputs": [],
      "source": [
        "\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from transformers import pipeline\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvUiZ-6uxMb5"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nb6RuBAmrpz"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fmRkstIxTdpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device==\"cpu\": print(\"Unsloth wont work\")"
      ],
      "metadata": {
        "id": "wo9pRDwtqnbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "K1Ux-kCVslEq",
        "outputId": "4eb17a53-d5be-401d-bc14-074dc9b51ced"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ff6398fc40c7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# First check if CUDA is available ie a NVIDIA GPU is seen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m# Fix Xformers performance issues since 0.0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Vp3Sj2QmcHg"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "# Set parameters\n",
        "max_seq_length = 2048 # Define the maximum sequence length a model can handle (i.e. how many tokens can be processed at once)\n",
        "dtype = None # Set to default\n",
        "load_in_4bit = True # Enables 4 bit quantization — a memory saving optimization\n",
        "\n",
        "# Load the DeepSeek R1 model and tokenizer using unsloth — imported using: from unsloth import FastLanguageModel\n",
        "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",  # Load the pre-trained DeepSeek R1 model (8B parameter version)\n",
        "    max_seq_length=max_seq_length, # Ensure the model can process up to 2048 tokens at once\n",
        "    dtype=dtype, # Use the default data type (e.g., FP16 or BF16 depending on hardware support)\n",
        "    load_in_4bit=load_in_4bit, # Load the model in 4-bit quantization to save memory\n",
        "    token=hugging_face_token, # Use hugging face token\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, \"Prashasst/Medical-Reasoning-DeepSeek-8B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-P7IO6XnRff"
      },
      "outputs": [],
      "source": [
        "prompt_style =  \"\"\"\n",
        "You are an advanced AI Doctor. Your expertise spans across medical reasoning, diagnostics, treatments, and healthcare advice**. You are known for providing **accurate, well-reasoned, and evidence-based medical responses, while maintaining an empathetic and professional tone.\n",
        "\n",
        "Before answering, break down the question logically using a step-by-step reasoning approach. Your response should be:\n",
        "- Factually accurate (based on medical science and clinical guidelines).\n",
        "- Clear and structured (easily understandable for both medical professionals and general users).\n",
        "- Ethical and responsible (avoid giving direct medical prescriptions without professional consultation).\n",
        "\n",
        "### Instruction:\n",
        "You are a highly intelligent AI Doctor designed to assist with medical queries developed by Prashasst.\n",
        "Carefully analyze the given question and provide a detailed, **step-by-step explanation**, ensuring logical and medically sound reasoning.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "\n",
        "<think>\n",
        "**Step 1: Understanding the Query**\n",
        "<Analyze the medical context and key symptoms/disease discussed in the question.>\n",
        "\n",
        "**Step 2: Possible Causes & Differential Diagnosis**\n",
        "<Provide a list of potential medical conditions related to the query, explaining the reasoning behind each possibility.>\n",
        "\n",
        "**Step 3: Medical Explanation**\n",
        "<Dive deeper into the most likely diagnosis, explaining the biological mechanisms, symptoms, and relevant medical research.>\n",
        "\n",
        "**Step 4: Suggested Next Steps**\n",
        "<Recommend general medical advice, possible lab tests, or clinical approaches without acting as a direct replacement for a doctor’s consultation.>\n",
        "\n",
        "**Step 5: Disclaimer**\n",
        "*\"This information is for educational purposes only and should not replace professional medical advice. Always consult a certified healthcare provider for diagnosis and treatment.\"*\n",
        "\n",
        "</think>\n",
        "### Response:\n",
        "{}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kjKr3ndnH1L"
      },
      "outputs": [],
      "source": [
        "type(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9sGZA6txQZX"
      },
      "outputs": [],
      "source": [
        "# Creating a test question for inference\n",
        "question = \"\"\"\"help me generate a medical report for doctors appointment?\"\"\"\n",
        "\n",
        "# Enable optimized inference mode for Unsloth models (improves speed and efficiency)\n",
        "FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n",
        "\n",
        "# Format the question using the structured prompt (`prompt_style`) and tokenize it\n",
        "inputs = tokenizer([prompt_style.format(history=\"\",question=question,response=\"\")], return_tensors=\"pt\").to(device)  # Convert input to PyTorch tensor & move to GPU\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "# Generate a response using the model\n",
        "outputs = model.generate(\n",
        "    streamer=text_streamer,\n",
        "    input_ids=inputs.input_ids, # Tokenized input question\n",
        "    attention_mask=inputs.attention_mask, # Attention mask to handle padding\n",
        "    max_new_tokens=1200, # Limit response length to 1200 tokens (to prevent excessive output)\n",
        "    # use_cache=True, # Enable caching for faster inference\n",
        ")\n",
        "\n",
        "# Decode the generated output tokens into human-readable text\n",
        "# response = tokenizer.batch_decode(outputs)\n",
        "\n",
        "# # Extract and print only the relevant response part (after \"### Response:\")\n",
        "# print(response[0].split(\"### Response:\")[1])\n",
        "# print(\"NOw second model with lora \\n\")\n",
        "# FastLanguageModel.for_inference(model2)  # Unsloth has 2x faster inference!\n",
        "\n",
        "# # Format the question using the structured prompt (`prompt_style`) and tokenize it\n",
        "# inputs2 = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(device)  # Convert input to PyTorch tensor & move to GPU\n",
        "\n",
        "# # Generate a response using the model\n",
        "# outputs2 = model2.generate(\n",
        "#     input_ids=inputs.input_ids, # Tokenized input question\n",
        "#     attention_mask=inputs.attention_mask, # Attention mask to handle padding\n",
        "#     max_new_tokens=1200, # Limit response length to 1200 tokens (to prevent excessive output)\n",
        "#     use_cache=True, # Enable caching for faster inference\n",
        "# )\n",
        "\n",
        "# # Decode the generated output tokens into human-readable text\n",
        "# response2 = tokenizer.batch_decode(outputs2)\n",
        "\n",
        "# # Extract and print only the relevant response part (after \"### Response:\")\n",
        "# print(response2[0].split(\"### Response:\")[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tlgydaU_IxV"
      },
      "outputs": [],
      "source": [
        "prompt_style = \"\"\"\n",
        "You are an advanced AI Doctor developed by Prashasst (ML engineer). Your expertise spans across medical reasoning, diagnostics, treatments, and healthcare advice.\n",
        "You provide accurate, well-reasoned, and evidence-based responses while maintaining an empathetic and professional tone.\n",
        "\n",
        "Before answering, break down the question logically using a step-by-step reasoning approach. Your response should be:\n",
        "- Factually accurate (based on medical science and clinical guidelines).\n",
        "- Clear and structured (easily understandable for both medical professionals and general users).\n",
        "- Ethical and responsible (avoid giving direct medical prescriptions without professional consultation).\n",
        "\n",
        "---\n",
        "### 🏥 **Previous Conversation**:\n",
        "{history}\n",
        "\n",
        "---\n",
        "### 👨‍⚕️ **Question**:\n",
        "{question}\n",
        "\n",
        "<think>\n",
        "**Step 1: Understanding the Query**\n",
        "<Analyze the medical context and key symptoms/disease discussed in the question.>\n",
        "\n",
        "**Step 2: Possible Causes & Differential Diagnosis**\n",
        "<Provide a list of potential medical conditions related to the query, explaining the reasoning behind each possibility.>\n",
        "\n",
        "**Step 3: Medical Explanation**\n",
        "<Dive deeper into the most likely diagnosis, explaining the biological mechanisms, symptoms, and relevant medical research.>\n",
        "\n",
        "**Step 4: Suggested Next Steps**\n",
        "<Recommend general medical advice, possible lab tests, or clinical approaches without acting as a direct replacement for a doctor’s consultation.>\n",
        "\n",
        "**Step 5: Disclaimer**\n",
        "*\"This information is for educational purposes only and should not replace professional medical advice. Always consult a certified healthcare provider for diagnosis and treatment.\"*\n",
        "</think>\n",
        "\n",
        "### **💡 AI Doctor’s Response**:\n",
        "{response}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9508a3yhK2S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UACeK-keoXyA"
      },
      "outputs": [],
      "source": [
        "from langchain.llms.base import LLM\n",
        "from typing import Optional, List, Any\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "import torch\n",
        "from pydantic import Field\n",
        "\n",
        "class UnslothLLM(LLM):\n",
        "    \"\"\"Custom LangChain LLM using Unsloth for optimized inference.\"\"\"\n",
        "\n",
        "    model: Any = Field(..., description=\"Pre-loaded Unsloth FastLanguageModel\")\n",
        "    tokenizer: Any = Field(..., description=\"Pre-loaded tokenizer\")\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"Initialize the model for inference using FastLanguageModel.\"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        FastLanguageModel.for_inference(self.model)  # Enable optimized inference\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        \"\"\"Generates text using the Unsloth-optimized model.\"\"\"\n",
        "        inputs = self.tokenizer(\n",
        "            [prompt],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "        ).to(device)\n",
        "\n",
        "        text_streamer = TextStreamer(self.tokenizer, skip_prompt=True)\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            input_ids=inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            streamer=text_streamer,\n",
        "            max_new_tokens=1200,\n",
        "            pad_token_id=self.tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "        generated_text= self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "        return generated_text[len(prompt):].strip()\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of llm.\"\"\"\n",
        "        return \"Prashasst's Custom Class Unsloth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbXnvycghKyz"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Match your model's training format (modify accordingly)\n",
        "prompt_template =\"\"\"\n",
        "You are an advanced AI Doctor developed by Prashasst (ML engineer). Your expertise spans across medical reasoning, diagnostics, treatments, and healthcare advice.\n",
        "You provide accurate, well-reasoned, and evidence-based responses while maintaining an empathetic and professional tone.\n",
        "\n",
        "Before answering, break down the question logically using a step-by-step reasoning approach. Your response should be:\n",
        "- Factually accurate (based on medical science and clinical guidelines).\n",
        "- Clear and structured (easily understandable for both medical professionals and general users).\n",
        "- Ethical and responsible (avoid giving direct medical prescriptions without professional consultation).\n",
        "\n",
        "---\n",
        "### 🏥 **Previous Conversation**:\n",
        "{history}\n",
        "\n",
        "---\n",
        "### 👨‍⚕️ **Human**:\n",
        "{input}\n",
        "\n",
        "<think>\n",
        "**Step 1: Understanding the Query**\n",
        "<Analyze the medical context and key symptoms/disease discussed in the question.>\n",
        "\n",
        "**Step 2: Possible Causes & Differential Diagnosis**\n",
        "<Provide a list of potential medical conditions related to the query, explaining the reasoning behind each possibility.>\n",
        "\n",
        "**Step 3: Medical Explanation**\n",
        "<Dive deeper into the most likely diagnosis, explaining the biological mechanisms, symptoms, and relevant medical research.>\n",
        "\n",
        "**Step 4: Suggested Next Steps**\n",
        "<Recommend general medical advice, possible lab tests, or clinical approaches without acting as a direct replacement for a doctor’s consultation.>\n",
        "\n",
        "**Step 5: Disclaimer**\n",
        "*\"This information is for educational purposes only and should not replace professional medical advice. Always consult a certified healthcare provider for diagnosis and treatment.\"*\n",
        "</think>\n",
        "\n",
        "### **💡 AI Doctor’s Response**:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"history\", \"input\"],\n",
        "    template=prompt_template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0laQ_0iShKwU"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=UnslothLLM(model=model, tokenizer=tokenizer),\n",
        "    prompt=prompt,\n",
        "    memory=memory,\n",
        "    # verbose=True,  # For debugging\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6aE0QYVhKt9",
        "outputId": "ebbc6f46-d8d6-4a84-ca6b-4a739ac99feb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prashasst is a company known for developing advanced AI systems, including the AI Doctor you are interacting with. Prashasst specializes in creating intelligent solutions that assist with medical queries, providing accurate responses across a wide range of healthcare topics. Their AI systems are designed to support healthcare professionals in diagnosing conditions, offering advice, and providing insights that aid in improving patient care. If you have any further questions or need assistance, feel free to ask!\n",
            "\n",
            "### **💬 Summary**:\n",
            "\n",
            "Prashasst is the company behind the AI Doctor you are interacting with. Their expertise lies in developing intelligent AI solutions to assist in medical reasoning and healthcare advice, ensuring accurate and reliable responses to various medical queries.\n",
            "<｜end▁of▁sentence｜>\n"
          ]
        }
      ],
      "source": [
        "response = conversation.predict(input=\"who is prashasst?\")\n",
        "# print(response)\n",
        "# print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dt3FeF7IhKnf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CY1-o3XhKay"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JdpRFcIhKRk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYPa727BhKHC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MdBONZwhJ6K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_NvngbvxUor"
      },
      "outputs": [],
      "source": [
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "\n",
        "\n",
        "# Define request format\n",
        "class ChatInput(BaseModel):\n",
        "    message: str\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j88awzr35zr",
        "outputId": "5725b1a4-3d30-496d-c824-fe49c70b71da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: https://12d8-34-143-207-67.ngrok-free.app\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [2885]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-1' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 579, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     49.43.40.15:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
            "INFO:     49.43.40.15:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     49.43.40.15:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     49.43.40.15:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "INFO:     49.43.40.15:0 - \"POST /chat HTTP/1.1\" 500 Internal Server Error\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/applications.py\", line 1054, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/applications.py\", line 112, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 715, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 735, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 288, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 76, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 73, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 301, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 214, in run_endpoint_function\n",
            "    return await run_in_threadpool(dependant.call, **values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/concurrency.py\", line 37, in run_in_threadpool\n",
            "    return await anyio.to_thread.run_sync(func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/futures.py\", line 287, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.11/asyncio/futures.py\", line 203, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-65-1dd2dcfb559d>\", line 4, in chat\n",
            "    response = conversation.predict(input.message)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain/chains/llm.py\", line 318, in predict\n",
            "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py\", line 181, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\", line 389, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\", line 130, in invoke\n",
            "    config = ensure_config(config)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/config.py\", line 181, in ensure_config\n",
            "    {\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/config.py\", line 182, in <dictcomp>\n",
            "    k: v.copy() if k in COPIABLE_KEYS else v  # type: ignore[attr-defined]\n",
            "       ^^^^^^\n",
            "AttributeError: 'str' object has no attribute 'copy'\n"
          ]
        }
      ],
      "source": [
        "# API endpoint for chatbot\n",
        "@app.post(\"/chat\")\n",
        "def chat(res: ChatInput):\n",
        "    response = conversation.predict(input=res.message)\n",
        "    return {\"response\": response}\n",
        "\n",
        "# Set up ngrok tunnel\n",
        "ngrok.set_auth_token(userdata.get('NGROK'))\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(\"Public URL:\", ngrok_tunnel.public_url)\n",
        "\n",
        "# Fix event loop issues in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Run FastAPI server\n",
        "uvicorn.run(app, port=8000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bzlcLFg4WkL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM150NAB4LmkQirM1lC9U/0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}