{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"authorship_tag":"ABX9TyM150NAB4LmkQirM1lC9U/0","include_colab_link":true},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/theprashasst/Medical-Reasoning-DeepSeek-R1/blob/main/prototyping_Intern_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"\n\n# %%capture\n\n# !pip install unsloth # install unsloth\n# !pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # Also get the latest version Unsloth!","metadata":{"id":"wJ--FDljmD5O","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T18:07:08.719649Z","iopub.execute_input":"2025-02-12T18:07:08.720049Z","iopub.status.idle":"2025-02-12T18:10:57.300743Z","shell.execute_reply.started":"2025-02-12T18:07:08.720007Z","shell.execute_reply":"2025-02-12T18:10:57.299454Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# from google.colab import userdata\n# hugging_face_token=userdata.get('HF_TOKEN')\n# ngrok_token=userdata.get('NGROK')\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhugging_face_token=user_secrets.get_secret(\"HF_TOKEN\")\nngrok_token=user_secrets.get_secret(\"NGROK\")\n\n","metadata":{"id":"4Ugx77sRss5S","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T18:55:21.118319Z","iopub.execute_input":"2025-02-12T18:55:21.118628Z","iopub.status.idle":"2025-02-12T18:55:21.516082Z","shell.execute_reply.started":"2025-02-12T18:55:21.118605Z","shell.execute_reply":"2025-02-12T18:55:21.515140Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"%%capture\n!pip install langchain_community transformers fastapi uvicorn pyngrok gradio\n","metadata":{"id":"kxDbF79x0DSO","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T18:55:25.101377Z","iopub.execute_input":"2025-02-12T18:55:25.101688Z","iopub.status.idle":"2025-02-12T18:55:47.150747Z","shell.execute_reply.started":"2025-02-12T18:55:25.101664Z","shell.execute_reply":"2025-02-12T18:55:47.149188Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n# from transformers import pipeline\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationChain\nfrom langchain.llms.huggingface_pipeline import HuggingFacePipeline\nfrom pyngrok import ngrok\nimport uvicorn\nimport nest_asyncio\nimport gradio as gr","metadata":{"id":"pQheZy7A3OKo","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T18:55:47.152231Z","iopub.execute_input":"2025-02-12T18:55:47.152552Z","iopub.status.idle":"2025-02-12T18:55:53.554202Z","shell.execute_reply.started":"2025-02-12T18:55:47.152525Z","shell.execute_reply":"2025-02-12T18:55:53.552863Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from transformers import TextIteratorStreamer\n# TextStreamer","metadata":{"id":"QvUiZ-6uxMb5","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T18:55:53.556239Z","iopub.execute_input":"2025-02-12T18:55:53.556919Z","iopub.status.idle":"2025-02-12T18:55:58.343643Z","shell.execute_reply.started":"2025-02-12T18:55:53.556882Z","shell.execute_reply":"2025-02-12T18:55:58.342411Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"","metadata":{"id":"-nb6RuBAmrpz","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T18:55:58.344978Z","iopub.execute_input":"2025-02-12T18:55:58.345605Z","iopub.status.idle":"2025-02-12T18:55:58.350440Z","shell.execute_reply.started":"2025-02-12T18:55:58.345572Z","shell.execute_reply":"2025-02-12T18:55:58.349087Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"","metadata":{"id":"fmRkstIxTdpD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif device==\"cpu\": \n    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n    print(\"Unsloth wont work\")\nelse:\n    %%capture\n\n    !pip install unsloth # install unsloth\n    !pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n    from unsloth import FastLanguageModel","metadata":{"id":"wo9pRDwtqnbI","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T18:58:59.974175Z","iopub.execute_input":"2025-02-12T18:58:59.974617Z","iopub.status.idle":"2025-02-12T18:59:00.323507Z","shell.execute_reply.started":"2025-02-12T18:58:59.974587Z","shell.execute_reply":"2025-02-12T18:59:00.322395Z"}},"outputs":[{"name":"stdout","text":"Unsloth wont work\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# from unsloth import FastLanguageModel","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":263},"id":"K1Ux-kCVslEq","outputId":"4eb17a53-d5be-401d-bc14-074dc9b51ced","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T18:11:25.762420Z","iopub.execute_input":"2025-02-12T18:11:25.762742Z","iopub.status.idle":"2025-02-12T18:11:57.904926Z","shell.execute_reply.started":"2025-02-12T18:11:25.762703Z","shell.execute_reply":"2025-02-12T18:11:57.904250Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"max_seq_length = 2048 # Define the maximum sequence length a model can handle (i.e. how many tokens can be processed at once)\ndtype = None # Set to default\nload_in_4bit = True # Enables 4 bit quantization — a memory saving optimization","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T18:56:04.683695Z","iopub.execute_input":"2025-02-12T18:56:04.684045Z","iopub.status.idle":"2025-02-12T18:56:04.688770Z","shell.execute_reply.started":"2025-02-12T18:56:04.684019Z","shell.execute_reply":"2025-02-12T18:56:04.687684Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\nfrom peft import PeftModel\n\ndef get_model(device,max_seq_length,dtype,load_in_4bit):\n    # Define the base model and adapter\n    adapter_name = \"Prashasst/Medical-Reasoning-DeepSeek-8B\" \n    \n    \n    \n    if device==\"cpu\":\n        base_model_name= \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n        # Load base model and tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n        base_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=\"auto\")\n        \n        # Load PEFT model\n        model = PeftModel.from_pretrained(base_model, adapter_name)\n    else:\n        # Set parameters\n        base_model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\"\n    \n    \n        # Load the DeepSeek R1 model and tokenizer using unsloth — imported using: from unsloth import FastLanguageModel\n        base_model, tokenizer = FastLanguageModel.from_pretrained(\n            model_name=base_model_name,  # Load the pre-trained DeepSeek R1 model (8B parameter version)\n            max_seq_length=max_seq_length, # Ensure the model can process up to 2048 tokens at once\n            dtype=dtype, # Use the default data type (e.g., FP16 or BF16 depending on hardware support)\n            load_in_4bit=load_in_4bit, # Load the model in 4-bit quantization to save memory\n            token=hugging_face_token, # Use hugging face token\n        )\n        model = PeftModel.from_pretrained(base_model, adapter_name)\n    return model,tokenizer\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T19:10:38.917122Z","iopub.execute_input":"2025-02-12T19:10:38.917550Z","iopub.status.idle":"2025-02-12T19:10:38.924311Z","shell.execute_reply.started":"2025-02-12T19:10:38.917519Z","shell.execute_reply":"2025-02-12T19:10:38.923047Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"model,tokenizer= get_model(device,max_seq_length,dtype,load_in_4bit)\n\n","metadata":{"id":"5Vp3Sj2QmcHg","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T19:10:51.393022Z","iopub.execute_input":"2025-02-12T19:10:51.393504Z","iopub.status.idle":"2025-02-12T19:11:35.689986Z","shell.execute_reply.started":"2025-02-12T19:10:51.393472Z","shell.execute_reply":"2025-02-12T19:11:35.688216Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3347d56bfe5949978ab9757b602002b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26b708926e9a4cc5a203ae1a1bfedef7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d703be89938649d48ec58af7b2aeb839"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b84eddf232c549108c460bb1fd7430f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6715457ab3442e29ff049a16a9e6642"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-000002.safetensors:   0%|          | 0.00/8.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c528cbca6a7445f9292141569eeb7b5"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-6c289042ac8a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mload_in_4bit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-173f3c1eb143>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(device, max_seq_length, dtype, load_in_4bit)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Load base model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Load PEFT model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3988\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_sharded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3989\u001b[0m             \u001b[0;31m# resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3990\u001b[0;31m             resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n\u001b[0m\u001b[1;32m   3991\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3992\u001b[0m                 \u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;31m# Load from URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m             cached_filename = cached_file(\n\u001b[0m\u001b[1;32m   1099\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mshard_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    858\u001b[0m         )\n\u001b[1;32m    859\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mWeakFileLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1009\u001b[0;31m         _download_to_tmp_and_move(\n\u001b[0m\u001b[1;32m   1010\u001b[0m             \u001b[0mincomplete_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".incomplete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0mdestination_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0m_check_disk_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m         http_get(\n\u001b[0m\u001b[1;32m   1544\u001b[0m             \u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0mnew_resume_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDOWNLOAD_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m                     \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    953\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mflush_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_error_catcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;31m# StringIO doesn't like amt=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m     def _raw_read(\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0;31m# clip the read to the \"end of response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0mamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":18},{"cell_type":"code","source":"prompt_style = \"\"\"\nYou are an advanced AI Doctor developed by Prashasst (ML engineer). Your expertise spans across medical reasoning, diagnostics, treatments, and healthcare advice.\nYou provide accurate, well-reasoned, and evidence-based responses while maintaining an empathetic and professional tone.\n\nBefore answering, break down the question logically using a step-by-step reasoning approach. Your response should be:\n- Factually accurate (based on medical science and clinical guidelines).\n- Clear and structured (easily understandable for both medical professionals and general users).\n- Ethical and responsible (avoid giving direct medical prescriptions without professional consultation).\n\n---\n### 🏥 **Previous Conversation**:\n{history}\n\n---\n### 👨‍⚕️ **Question**:\n{question}\n\n<think>\n**Step 1: Understanding the Query**\n<Analyze the medical context and key symptoms/disease discussed in the question.>\n\n**Step 2: Possible Causes & Differential Diagnosis**\n<Provide a list of potential medical conditions related to the query, explaining the reasoning behind each possibility.>\n\n**Step 3: Medical Explanation**\n<Dive deeper into the most likely diagnosis, explaining the biological mechanisms, symptoms, and relevant medical research.>\n\n**Step 4: Suggested Next Steps**\n<Recommend general medical advice, possible lab tests, or clinical approaches without acting as a direct replacement for a doctor’s consultation.>\n\n**Step 5: Disclaimer**\n*\"This information is for educational purposes only and should not replace professional medical advice. Always consult a certified healthcare provider for diagnosis and treatment.\"*\n</think>\n\n### **💡 AI Doctor’s Response**:\n{response}\n\"\"\"\n","metadata":{"id":"3-P7IO6XnRff","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T18:14:44.972547Z","iopub.execute_input":"2025-02-12T18:14:44.972860Z","iopub.status.idle":"2025-02-12T18:14:44.977191Z","shell.execute_reply.started":"2025-02-12T18:14:44.972831Z","shell.execute_reply":"2025-02-12T18:14:44.976347Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# type(tokenizer)","metadata":{"id":"0kjKr3ndnH1L","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T18:14:44.978035Z","iopub.execute_input":"2025-02-12T18:14:44.978277Z","iopub.status.idle":"2025-02-12T18:14:46.484314Z","shell.execute_reply.started":"2025-02-12T18:14:44.978247Z","shell.execute_reply":"2025-02-12T18:14:46.483541Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# # Creating a test question for inference\n# question = \"\"\"\"help me generate a medical report for doctors appointment?\"\"\"\n\n# # Enable optimized inference mode for Unsloth models (improves speed and efficiency)\n# FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n\n# # Format the question using the structured prompt (`prompt_style`) and tokenize it\n# inputs = tokenizer([prompt_style.format(history=\"\",question=question,response=\"\")], return_tensors=\"pt\").to(device)  # Convert input to PyTorch tensor & move to GPU\n# text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n# # Generate a response using the model\n# outputs = model.generate(\n#     streamer=text_streamer,\n#     input_ids=inputs.input_ids, # Tokenized input question\n#     attention_mask=inputs.attention_mask, # Attention mask to handle padding\n#     max_new_tokens=1200, # Limit response length to 1200 tokens (to prevent excessive output)\n#     # use_cache=True, # Enable caching for faster inference\n# )\n\n# # Decode the generated output tokens into human-readable text\n# # response = tokenizer.batch_decode(outputs)\n\n# # # Extract and print only the relevant response part (after \"### Response:\")\n# # print(response[0].split(\"### Response:\")[1])\n# # print(\"NOw second model with lora \\n\")\n# # FastLanguageModel.for_inference(model2)  # Unsloth has 2x faster inference!\n\n# # # Format the question using the structured prompt (`prompt_style`) and tokenize it\n# # inputs2 = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(device)  # Convert input to PyTorch tensor & move to GPU\n\n# # # Generate a response using the model\n# # outputs2 = model2.generate(\n# #     input_ids=inputs.input_ids, # Tokenized input question\n# #     attention_mask=inputs.attention_mask, # Attention mask to handle padding\n# #     max_new_tokens=1200, # Limit response length to 1200 tokens (to prevent excessive output)\n# #     use_cache=True, # Enable caching for faster inference\n# # )\n\n# # # Decode the generated output tokens into human-readable text\n# # response2 = tokenizer.batch_decode(outputs2)\n\n# # # Extract and print only the relevant response part (after \"### Response:\")\n# # print(response2[0].split(\"### Response:\")[1])","metadata":{"id":"Q9sGZA6txQZX","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T18:14:46.485093Z","iopub.execute_input":"2025-02-12T18:14:46.485323Z","iopub.status.idle":"2025-02-12T18:14:46.502231Z","shell.execute_reply.started":"2025-02-12T18:14:46.485300Z","shell.execute_reply":"2025-02-12T18:14:46.501449Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"","metadata":{"id":"0tlgydaU_IxV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.callbacks.manager import BaseRunManager\nfrom typing import List\n\nclass StreamingRunManager(BaseRunManager):\n    def __init__(self, run_id, handlers=None, inheritable_handlers=None):\n        super().__init__(run_id=run_id, handlers=handlers or [], inheritable_handlers=inheritable_handlers or [])\n        self.tokens: List[str] = []  # Store streamed tokens\n\n    def on_llm_new_token(self, token: str) -> None:\n        \"\"\"Capture streamed tokens and store them.\"\"\"\n        self.tokens.append(token)\n","metadata":{"id":"G9508a3yhK2S","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T18:14:46.503099Z","iopub.execute_input":"2025-02-12T18:14:46.503357Z","iopub.status.idle":"2025-02-12T18:14:46.527013Z","shell.execute_reply.started":"2025-02-12T18:14:46.503337Z","shell.execute_reply":"2025-02-12T18:14:46.526340Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from langchain.llms.base import LLM\nfrom langchain.callbacks.manager import CallbackManagerForLLMRun\nfrom langchain.schema import LLMResult\nfrom typing import Optional, List, Any\nfrom unsloth import FastLanguageModel\nfrom transformers import TextStreamer\nimport torch\nfrom pydantic import Field\nfrom threading import Thread\n\nclass UnslothLLM(LLM):\n    \"\"\"Custom LangChain LLM using Unsloth for optimized inference.\"\"\"\n\n    model: Any = Field(..., description=\"Pre-loaded Unsloth FastLanguageModel or CasualLLM\")\n    tokenizer: Any = Field(..., description=\"Pre-loaded tokenizer\")\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the model for inference using FastLanguageModel.\"\"\"\n        super().__init__(**kwargs)\n        if torch.cuda.is_available():\n            FastLanguageModel.for_inference(self.model)  # Enable optimized inference\n\n    def _call(self, prompt: str,stop: Optional[List[str]] = None,\n        run_manager: Optional[StreamingRunManager] = None) -> str:\n        \"\"\"Generates text using the Unsloth-optimized model.\"\"\"\n        inputs = self.tokenizer(\n            [prompt],\n            return_tensors=\"pt\",\n            padding=True,\n        ).to(device)\n\n        streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True)\n       \n        thread = Thread(\n        target=self.model.generate,\n        kwargs={\n            \"input_ids\": inputs.input_ids,\n            \"attention_mask\": inputs.attention_mask,\n            \"streamer\": streamer,\n            \"max_new_tokens\": 540,\n            \"pad_token_id\": self.tokenizer.eos_token_id,\n            })\n            # ,daemon=True)\n        thread.start()\n    \n        # Store generated tokens to reconstruct full response\n        full_response = \"\"\n        \n        # for token in streamer:\n        #     full_response += token\n        #     yield token  # Stream to Gradio in real time\n        for new_text in streamer:\n            full_response += new_text\n            if run_manager:  # Stream token-by-token to LangChain\n                # print(new_text)\n                run_manager.on_llm_new_token(new_text)\n        # thread.join()\n        if full_response.startswith(prompt):\n            full_response = full_response[len(prompt):].strip()\n        # generated_text= self.tokenizer.decode(full_response, skip_special_tokens=True).strip()\n        # return generated_text\n        return full_response\n\n    \n    \n        # Remove any unintended parts (prompt, system instructions)\n        # if full_response.startswith(prompt):\n        #     full_response = full_response[len(prompt):].strip()\n    \n        # return full_response\n\n        # text_streamer = TextStreamer(self.tokenizer, skip_prompt=True)\n\n        # outputs = self.model.generate(\n        #     input_ids=inputs.input_ids,\n        #     attention_mask=inputs.attention_mask,\n        #     streamer=text_streamer,\n        #     max_new_tokens=1200,\n        #     pad_token_id=self.tokenizer.eos_token_id,\n        # )\n\n        # generated_text= self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n        # return generated_text[len(prompt):].strip()\n\n    @property\n    def _llm_type(self) -> str:\n        \"\"\"Return type of llm.\"\"\"\n        return \"Prashasst's Custom Class Unsloth\"","metadata":{"id":"UACeK-keoXyA","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T18:34:01.122429Z","iopub.execute_input":"2025-02-12T18:34:01.122771Z","iopub.status.idle":"2025-02-12T18:34:01.136127Z","shell.execute_reply.started":"2025-02-12T18:34:01.122744Z","shell.execute_reply":"2025-02-12T18:34:01.134964Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\n\n# Match your model's training format (modify accordingly)\nprompt_template =\"\"\"\nYou are an advanced AI Doctor developed by Prashasst (ML engineer). Your expertise spans across medical reasoning, diagnostics, treatments, and healthcare advice.\nYou provide accurate, well-reasoned, and evidence-based responses while maintaining an empathetic and professional tone.\n\nBefore answering, break down the question logically using a step-by-step reasoning approach. Your response should be:\n- Factually accurate (based on medical science and clinical guidelines).\n- Clear and structured (easily understandable for both medical professionals and general users).\n- Ethical and responsible (avoid giving direct medical prescriptions without professional consultation).\n\n---\n### 🏥 **Previous Conversation**:\n{history}\n\n---\n### 👨‍⚕️ **Human**:\n{input}\n\n<think>\n**Step 1: Understanding the Query**\n<Analyze the medical context and key symptoms/disease discussed in the question.>\n\n**Step 2: Possible Causes & Differential Diagnosis**\n<Provide a list of potential medical conditions related to the query, explaining the reasoning behind each possibility.>\n\n**Step 3: Medical Explanation**\n<Dive deeper into the most likely diagnosis, explaining the biological mechanisms, symptoms, and relevant medical research.>\n\n**Step 4: Suggested Next Steps**\n<Recommend general medical advice, possible lab tests, or clinical approaches without acting as a direct replacement for a doctor’s consultation.>\n\n**Step 5: Disclaimer**\n*\"This information is for educational purposes only and should not replace professional medical advice. Always consult a certified healthcare provider for diagnosis and treatment.\"*\n</think>\n\n### **💡 AI Doctor’s Response**:\n\n\"\"\"\n\n\nprompt = PromptTemplate(\n    input_variables=[\"history\", \"input\"],\n    template=prompt_template,\n)","metadata":{"id":"GbXnvycghKyz","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T18:34:01.501825Z","iopub.execute_input":"2025-02-12T18:34:01.502152Z","iopub.status.idle":"2025-02-12T18:34:01.507406Z","shell.execute_reply.started":"2025-02-12T18:34:01.502113Z","shell.execute_reply":"2025-02-12T18:34:01.506481Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"from langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferMemory\n\nmemory = ConversationBufferMemory()\n\nconversation = ConversationChain(\n    llm=UnslothLLM(model=model, tokenizer=tokenizer),\n    prompt=prompt,\n    memory=memory,\n    # verbose=True,  # For debugging\n)","metadata":{"id":"0laQ_0iShKwU","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T18:34:06.269293Z","iopub.execute_input":"2025-02-12T18:34:06.269618Z","iopub.status.idle":"2025-02-12T18:34:06.274861Z","shell.execute_reply.started":"2025-02-12T18:34:06.269590Z","shell.execute_reply":"2025-02-12T18:34:06.273926Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# response = conversation.predict(input=\"who is prashasst?\")\n# print(response)\n# print(response)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D6aE0QYVhKt9","outputId":"ebbc6f46-d8d6-4a84-ca6b-4a739ac99feb","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T18:20:27.901964Z","iopub.execute_input":"2025-02-12T18:20:27.902242Z","iopub.status.idle":"2025-02-12T18:20:53.196052Z","shell.execute_reply.started":"2025-02-12T18:20:27.902221Z","shell.execute_reply":"2025-02-12T18:20:53.195269Z"}},"outputs":[{"name":"stdout","text":"Prashasst is an advanced AI Doctor, developed by Prashasst (a company known for AI-driven healthcare solutions). Prashasst is designed to assist with medical reasoning, diagnostics, treatments, and healthcare advice, ensuring accurate and empathetic responses, similar to what a skilled doctor would provide. \n\n### **🔍 Context**:\n\nPrashasst is part of a broader AI revolution in healthcare, where AI systems like Prashasst are used to enhance medical decision-making, improve diagnostics, and offer guidance to healthcare professionals. These AI tools aim to make healthcare more accessible, accurate, and efficient.\n\n### **📊 Key Feature**:\n\nOne notable feature of Prashasst is its ability to provide **accurate, well-reasoned, and evidence-based medical responses, while maintaining an empathetic and professional tone. This makes Prashasst a reliable resource for both medical professionals and general users seeking healthcare advice.\n\n### **💡 How It Works**:\n\nPrashasst uses a combination of **machine learning algorithms, natural language processing, and domain expertise** to answer queries developed by Prashasst (the company). It is designed to assist with a wide range of medical queries, from diagnostics and treatments to healthcare advice and general medical reasoning.\n\n### **🔎 Example Query Handling**:\n\nIf you have a medical question, Prashasst can help with:\n\n- **Disease Diagnosis**: By providing a differential diagnosis based on symptoms and relevant medical research.\n- **Treatment Recommendations**: Offering guidance on potential medical treatments and approaches.\n- **Health Advice**: Answering general health queries and suggesting next steps without acting as a direct replacement for a doctor’s consultation.\n\n### **💡 Disclaimer**:\n\n*\"This information is for educational purposes only and should not replace professional medical advice. Always consult a certified healthcare provider for diagnosis and treatment.\"*\n\nIf you have any medical-related questions, feel free to ask Prashasst for assistance!\n<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import time\nimport uuid  # To generate unique IDs\nimport threading  # Import threading\n\n# Define request format\nclass ChatInput(BaseModel):\n    message: str\n\ndef AI_doc(message: ChatInput, history):\n    run_manager = StreamingRunManager(run_id=str(uuid.uuid4()))  # Generate a unique run ID\n\n    # Run the LLM in a separate thread\n    response_thread = threading.Thread(\n        target=conversation.llm._call,\n        args=(message,),  # Correctly pass message content\n        kwargs={\"run_manager\": run_manager} # Pass run_manager\n        # daemon=True\n    )\n    response_thread.start()\n\n    # Yield tokens in real time\n    prev_len = 0\n    outt = \"\"\n\n    while response_thread.is_alive() or prev_len < len(run_manager.tokens):\n        if len(run_manager.tokens) > prev_len:\n            outt += run_manager.tokens[prev_len]\n            prev_len += 1\n            yield outt\n        else:\n            time.sleep(0.05)  # Prevent CPU overuse\n\n    response_thread.join()  # Ensure the thread has finished execution\n","metadata":{"id":"dt3FeF7IhKnf","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T18:34:11.427766Z","iopub.execute_input":"2025-02-12T18:34:11.428089Z","iopub.status.idle":"2025-02-12T18:34:11.437112Z","shell.execute_reply.started":"2025-02-12T18:34:11.428063Z","shell.execute_reply":"2025-02-12T18:34:11.435765Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# conversation = ConversationChain(\n#     llm=UnslothLLM(model=model, tokenizer=tokenizer),\n#     prompt=prompt,\n#     memory=memory,\n#     # verbose=True,  # For debugging\n# )\n# def AI_doc( message:ChatInput , history):\n#     response = \"\"  # Store full response\n    \n#     # Run the conversation chain to get the streamer\n#     for new_text in conversation.predict(input=message):  \n#         response += new_text  \n#         yield response\n#     # response=conversation.predict(input=message)\n#     # return response\n#     # for token in llm._call(prompt, run_manager=None):  # ⚡ Calls the model\n#     #     yield token  # 🔥 Streams each token to Gradio in real-time\n\n\n    \n    ","metadata":{"id":"4CY1-o3XhKay","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T17:19:36.320612Z","iopub.execute_input":"2025-02-12T17:19:36.320970Z","iopub.status.idle":"2025-02-12T17:19:36.325596Z","shell.execute_reply.started":"2025-02-12T17:19:36.320943Z","shell.execute_reply":"2025-02-12T17:19:36.324737Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"app=gr.ChatInterface(AI_doc )\napp.launch()","metadata":{"id":"6JdpRFcIhKRk","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T18:34:25.384963Z","iopub.execute_input":"2025-02-12T18:34:25.385286Z","iopub.status.idle":"2025-02-12T18:34:26.513418Z","shell.execute_reply.started":"2025-02-12T18:34:25.385260Z","shell.execute_reply":"2025-02-12T18:34:26.512522Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:290: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7862\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://8e337a2f2c584c3d93.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://8e337a2f2c584c3d93.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"name":"stderr","text":"Exception in thread Thread-50 (_fast_generate):\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py\", line 1562, in _fast_generate\n    output = generate(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 1838, in generate\n    outputs = self.base_model.generate(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2255, in generate\n    result = self._sample(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 3257, in _sample\n    outputs = model_forward(**model_inputs, return_dict=True)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py\", line 1025, in _CausalLM_fast_forward\n    outputs = fast_forward_inference(\n  File \"/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py\", line 961, in LlamaModel_fast_forward_inference\n    X, present_key_value = LlamaAttention_fast_forward_inference(\n  File \"/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py\", line 231, in LlamaAttention_fast_forward_inference\n    self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1928, in __getattr__\n    raise AttributeError(\nAttributeError: 'LlamaAttention' object has no attribute 'paged_attention_K'\nException in thread Thread-48 (_fast_generate):\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\nException in thread Thread-52 (_fast_generate):\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n        self.run()self.run()\n  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n\n  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py\", line 1562, in _fast_generate\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py\", line 1562, in _fast_generate\n    output = generate(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 1838, in generate\n    output = generate(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 1838, in generate\n        outputs = self.base_model.generate(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\noutputs = self.base_model.generate(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2255, in generate\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2255, in generate\n    result = self._sample(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 3311, in _sample\n        streamer.put(next_tokens.cpu())\nRuntimeError: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nresult = self._sample(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 3254, in _sample\n    outputs = self(**model_inputs, return_dict=True)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py\", line 1043, in _CausalLM_fast_forward\n    outputs = self.model(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py\", line 868, in LlamaModel_fast_forward\n    layer_outputs = decoder_layer(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py\", line 539, in LlamaDecoderLayer_fast_forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 189, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/bnb.py\", line 518, in forward\n    output = lora_B(lora_A(dropout(x))) * scaling\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)\nRuntimeError: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"","metadata":{"id":"WYPa727BhKHC","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"0MdBONZwhJ6K","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize FastAPI app\n# app = FastAPI()\n\n\n\n\n\n","metadata":{"id":"C_NvngbvxUor","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # API endpoint for chatbot\n# @app.post(\"/chat\")\n# def chat(res: ChatInput):\n#     response = conversation.predict(input=res.message)\n#     return {\"response\": response}\n\n# # Set up ngrok tunnel\n# ngrok.set_auth_token(ngrok_token)\n# ngrok_tunnel = ngrok.connect(8000)\n# print(\"Public URL:\", ngrok_tunnel.public_url)\n\n# # Fix event loop issues in Colab\n# nest_asyncio.apply()\n\n# # Run FastAPI server\n# uvicorn.run(app, port=8000)\n","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"5j88awzr35zr","outputId":"5725b1a4-3d30-496d-c824-fe49c70b71da","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"0bzlcLFg4WkL","trusted":true},"outputs":[],"execution_count":null}]}