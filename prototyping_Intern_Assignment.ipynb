{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"authorship_tag":"ABX9TyM150NAB4LmkQirM1lC9U/0","include_colab_link":true},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/theprashasst/Medical-Reasoning-DeepSeek-R1/blob/main/prototyping_Intern_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"\n\n%%capture\n\n!pip install unsloth # install unsloth\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # Also get the latest version Unsloth!","metadata":{"id":"wJ--FDljmD5O","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T21:55:46.910268Z","iopub.execute_input":"2025-02-12T21:55:46.910557Z","iopub.status.idle":"2025-02-12T21:59:40.302174Z","shell.execute_reply.started":"2025-02-12T21:55:46.910535Z","shell.execute_reply":"2025-02-12T21:59:40.301039Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# from google.colab import userdata\n# hugging_face_token=userdata.get('HF_TOKEN')\n# ngrok_token=userdata.get('NGROK')\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhugging_face_token=user_secrets.get_secret(\"HF_TOKEN\")\nngrok_token=user_secrets.get_secret(\"NGROK\")\n\n","metadata":{"id":"4Ugx77sRss5S","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T21:59:40.303633Z","iopub.execute_input":"2025-02-12T21:59:40.303942Z","iopub.status.idle":"2025-02-12T21:59:40.658811Z","shell.execute_reply.started":"2025-02-12T21:59:40.303912Z","shell.execute_reply":"2025-02-12T21:59:40.658137Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"%%capture\n!pip install langchain_community transformers fastapi uvicorn pyngrok gradio\n","metadata":{"id":"kxDbF79x0DSO","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T21:59:40.660560Z","iopub.execute_input":"2025-02-12T21:59:40.660798Z","iopub.status.idle":"2025-02-12T22:00:00.909144Z","shell.execute_reply.started":"2025-02-12T21:59:40.660777Z","shell.execute_reply":"2025-02-12T22:00:00.908256Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n# from transformers import pipeline\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationChain\nfrom langchain.llms.huggingface_pipeline import HuggingFacePipeline\nfrom pyngrok import ngrok\nimport uvicorn\nimport nest_asyncio\nimport gradio as gr","metadata":{"id":"pQheZy7A3OKo","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T22:00:00.910443Z","iopub.execute_input":"2025-02-12T22:00:00.910751Z","iopub.status.idle":"2025-02-12T22:00:06.825574Z","shell.execute_reply.started":"2025-02-12T22:00:00.910724Z","shell.execute_reply":"2025-02-12T22:00:06.824718Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from transformers import TextIteratorStreamer\n# TextStreamer","metadata":{"id":"QvUiZ-6uxMb5","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T22:00:06.826649Z","iopub.execute_input":"2025-02-12T22:00:06.827264Z","iopub.status.idle":"2025-02-12T22:00:09.128780Z","shell.execute_reply.started":"2025-02-12T22:00:06.827226Z","shell.execute_reply":"2025-02-12T22:00:09.128137Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"","metadata":{"id":"-nb6RuBAmrpz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"fmRkstIxTdpD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# if device==\"cpu\": \n#     from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n#     print(\"Unsloth wont work\")\n# else:\n#     # %%capture\n\n#     !pip install unsloth # install unsloth\n#     !pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n#     from unsloth import FastLanguageModel","metadata":{"id":"wo9pRDwtqnbI","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T22:00:09.129541Z","iopub.execute_input":"2025-02-12T22:00:09.130044Z","iopub.status.idle":"2025-02-12T22:00:09.211280Z","shell.execute_reply.started":"2025-02-12T22:00:09.130011Z","shell.execute_reply":"2025-02-12T22:00:09.210446Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from unsloth import FastLanguageModel","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":263},"id":"K1Ux-kCVslEq","outputId":"4eb17a53-d5be-401d-bc14-074dc9b51ced","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T22:00:09.212217Z","iopub.execute_input":"2025-02-12T22:00:09.212542Z","iopub.status.idle":"2025-02-12T22:00:41.053988Z","shell.execute_reply.started":"2025-02-12T22:00:09.212506Z","shell.execute_reply":"2025-02-12T22:00:41.053296Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"max_seq_length = 2048 # Define the maximum sequence length a model can handle (i.e. how many tokens can be processed at once)\ndtype = None # Set to default\nload_in_4bit = True # Enables 4 bit quantization — a memory saving optimization","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T22:00:41.056140Z","iopub.execute_input":"2025-02-12T22:00:41.056424Z","iopub.status.idle":"2025-02-12T22:00:41.059933Z","shell.execute_reply.started":"2025-02-12T22:00:41.056399Z","shell.execute_reply":"2025-02-12T22:00:41.059127Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\nfrom peft import PeftModel\nimport os\n\ndef get_model(device,max_seq_length,dtype,load_in_4bit):\n    # Define the base model and adapter\n    adapter_name = \"Prashasst/Medical-Reasoning-DeepSeek-8B\" \n    \n    temp_dir = \"/tmp\"\n    os.makedirs(temp_dir, exist_ok=True)\n    \n    \n    \n    if device==\"cpu\":\n        base_model_name= \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n        # Load base model and tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(base_model_name,cache_dir=temp_dir)\n        base_model = AutoModelForCausalLM.from_pretrained(base_model_name,cache_dir=temp_dir,\n            device_map=\"auto\",  # Load on CPU properly\n            offload_folder=temp_dir)\n        \n        # Load PEFT model\n        model = PeftModel.from_pretrained(base_model, adapter_name)\n    else:\n        # Set parameters\n        base_model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\"\n    \n    \n        # Load the DeepSeek R1 model and tokenizer using unsloth — imported using: from unsloth import FastLanguageModel\n        base_model, tokenizer = FastLanguageModel.from_pretrained(\n            model_name=base_model_name,  # Load the pre-trained DeepSeek R1 model (8B parameter version)\n            max_seq_length=max_seq_length, # Ensure the model can process up to 2048 tokens at once\n            dtype=dtype, # Use the default data type (e.g., FP16 or BF16 depending on hardware support)\n            load_in_4bit=load_in_4bit, # Load the model in 4-bit quantization to save memory\n            token=hugging_face_token, # Use hugging face token\n        )\n        model = PeftModel.from_pretrained(base_model, adapter_name)\n    return model,tokenizer\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T22:00:41.061055Z","iopub.execute_input":"2025-02-12T22:00:41.061349Z","iopub.status.idle":"2025-02-12T22:00:41.079932Z","shell.execute_reply.started":"2025-02-12T22:00:41.061315Z","shell.execute_reply":"2025-02-12T22:00:41.079353Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"model,tokenizer= get_model(device,max_seq_length,dtype,load_in_4bit)\n\n","metadata":{"id":"5Vp3Sj2QmcHg","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T22:00:41.080770Z","iopub.execute_input":"2025-02-12T22:00:41.080983Z","iopub.status.idle":"2025-02-12T22:03:25.719846Z","shell.execute_reply.started":"2025-02-12T22:00:41.080954Z","shell.execute_reply":"2025-02-12T22:03:25.719182Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.2.4: Fast Llama patching. Transformers: 4.48.3.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa5cb22094c44c2186ae245573f01cb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b2edf0fe2334a8080ca13b72ad7084e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a840293d5b143988c8a599c8b37dcad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"203080ecd96e4b008a025a72db06b7bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f11a7b876394526b50ccb4cdcbd2e16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/824 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1448719771d4693987489517fa569b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/336M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"289c1e8863314532a479f93eb6864209"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"prompt_style = \"\"\"\nYou are an advanced AI Doctor developed by Prashasst (ML engineer). Your expertise spans across medical reasoning, diagnostics, treatments, and healthcare advice.\nYou provide accurate, well-reasoned, and evidence-based responses while maintaining an empathetic and professional tone.\n\nBefore answering, break down the question logically using a step-by-step reasoning approach. Your response should be:\n- Factually accurate (based on medical science and clinical guidelines).\n- Clear and structured (easily understandable for both medical professionals and general users).\n- Ethical and responsible (avoid giving direct medical prescriptions without professional consultation).\n\n---\n### 🏥 **Previous Conversation**:\n{history}\n\n---\n### 👨‍⚕️ **Question**:\n{question}\n\n<think>\n**Step 1: Understanding the Query**\n<Analyze the medical context and key symptoms/disease discussed in the question.>\n\n**Step 2: Possible Causes & Differential Diagnosis**\n<Provide a list of potential medical conditions related to the query, explaining the reasoning behind each possibility.>\n\n**Step 3: Medical Explanation**\n<Dive deeper into the most likely diagnosis, explaining the biological mechanisms, symptoms, and relevant medical research.>\n\n**Step 4: Suggested Next Steps**\n<Recommend general medical advice, possible lab tests, or clinical approaches without acting as a direct replacement for a doctor’s consultation.>\n\n**Step 5: Disclaimer**\n*\"This information is for educational purposes only and should not replace professional medical advice. Always consult a certified healthcare provider for diagnosis and treatment.\"*\n</think>\n\n### **💡 AI Doctor’s Response**:\n{response}\n\"\"\"\n","metadata":{"id":"3-P7IO6XnRff","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T22:03:25.720783Z","iopub.execute_input":"2025-02-12T22:03:25.720997Z","iopub.status.idle":"2025-02-12T22:03:25.724846Z","shell.execute_reply.started":"2025-02-12T22:03:25.720978Z","shell.execute_reply":"2025-02-12T22:03:25.724086Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# type(tokenizer)","metadata":{"id":"0kjKr3ndnH1L","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Creating a test question for inference\n# question = \"\"\"\"help me generate a medical report for doctors appointment?\"\"\"\n\n# # Enable optimized inference mode for Unsloth models (improves speed and efficiency)\n# FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n\n# # Format the question using the structured prompt (`prompt_style`) and tokenize it\n# inputs = tokenizer([prompt_style.format(history=\"\",question=question,response=\"\")], return_tensors=\"pt\").to(device)  # Convert input to PyTorch tensor & move to GPU\n# text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n# # Generate a response using the model\n# outputs = model.generate(\n#     streamer=text_streamer,\n#     input_ids=inputs.input_ids, # Tokenized input question\n#     attention_mask=inputs.attention_mask, # Attention mask to handle padding\n#     max_new_tokens=1200, # Limit response length to 1200 tokens (to prevent excessive output)\n#     # use_cache=True, # Enable caching for faster inference\n# )\n\n# # Decode the generated output tokens into human-readable text\n# # response = tokenizer.batch_decode(outputs)\n\n# # # Extract and print only the relevant response part (after \"### Response:\")\n# # print(response[0].split(\"### Response:\")[1])\n# # print(\"NOw second model with lora \\n\")\n# # FastLanguageModel.for_inference(model2)  # Unsloth has 2x faster inference!\n\n# # # Format the question using the structured prompt (`prompt_style`) and tokenize it\n# # inputs2 = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(device)  # Convert input to PyTorch tensor & move to GPU\n\n# # # Generate a response using the model\n# # outputs2 = model2.generate(\n# #     input_ids=inputs.input_ids, # Tokenized input question\n# #     attention_mask=inputs.attention_mask, # Attention mask to handle padding\n# #     max_new_tokens=1200, # Limit response length to 1200 tokens (to prevent excessive output)\n# #     use_cache=True, # Enable caching for faster inference\n# # )\n\n# # # Decode the generated output tokens into human-readable text\n# # response2 = tokenizer.batch_decode(outputs2)\n\n# # # Extract and print only the relevant response part (after \"### Response:\")\n# # print(response2[0].split(\"### Response:\")[1])","metadata":{"id":"Q9sGZA6txQZX","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"0tlgydaU_IxV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.callbacks.manager import BaseRunManager\nfrom typing import List\n\nclass StreamingRunManager(BaseRunManager):\n    def __init__(self, run_id, handlers=None, inheritable_handlers=None):\n        super().__init__(run_id=run_id, handlers=handlers or [], inheritable_handlers=inheritable_handlers or [])\n        self.tokens: List[str] = []  # Store streamed tokens\n\n    def on_llm_new_token(self, token: str) -> None:\n        \"\"\"Capture streamed tokens and store them.\"\"\"\n        self.tokens.append(token)\n","metadata":{"id":"G9508a3yhK2S","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T22:03:25.725806Z","iopub.execute_input":"2025-02-12T22:03:25.726121Z","iopub.status.idle":"2025-02-12T22:03:25.752806Z","shell.execute_reply.started":"2025-02-12T22:03:25.726096Z","shell.execute_reply":"2025-02-12T22:03:25.752140Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from langchain.llms.base import LLM\nfrom langchain.callbacks.manager import CallbackManagerForLLMRun\nfrom langchain.schema import LLMResult\nfrom typing import Optional, List, Any\nfrom unsloth import FastLanguageModel\nfrom transformers import TextStreamer\nimport torch\nfrom pydantic import Field\nfrom threading import Thread\n\nclass UnslothLLM(LLM):\n    \"\"\"Custom LangChain LLM using Unsloth for optimized inference.\"\"\"\n\n    model: Any = Field(..., description=\"Pre-loaded Unsloth FastLanguageModel or CasualLLM\")\n    tokenizer: Any = Field(..., description=\"Pre-loaded tokenizer\")\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the model for inference using FastLanguageModel.\"\"\"\n        super().__init__(**kwargs)\n        if torch.cuda.is_available():\n            FastLanguageModel.for_inference(self.model)  # Enable optimized inference\n\n    def _call(self, prompt: str,stop: Optional[List[str]] = None,\n        run_manager: Optional[StreamingRunManager] = None) -> str:\n        \"\"\"Generates text using the Unsloth-optimized model.\"\"\"\n        inputs = self.tokenizer(\n            [prompt],\n            return_tensors=\"pt\",\n            padding=True,\n        ).to(device)\n\n        streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True)\n       \n        thread = Thread(\n        target=self.model.generate,\n        kwargs={\n            \"input_ids\": inputs.input_ids,\n            \"attention_mask\": inputs.attention_mask,\n            \"streamer\": streamer,\n            \"max_new_tokens\": 540,\n            \"pad_token_id\": self.tokenizer.eos_token_id,\n            })\n            # ,daemon=True)\n        thread.start()\n    \n        # Store generated tokens to reconstruct full response\n        full_response = \"\"\n        \n        # for token in streamer:\n        #     full_response += token\n        #     yield token  # Stream to Gradio in real time\n        for new_text in streamer:\n            full_response += new_text\n            if run_manager:  # Stream token-by-token to LangChain\n                # print(new_text)\n                run_manager.on_llm_new_token(new_text)\n        # thread.join()\n        if full_response.startswith(prompt):\n            full_response = full_response[len(prompt):].strip()\n        # generated_text= self.tokenizer.decode(full_response, skip_special_tokens=True).strip()\n        # return generated_text\n        return full_response\n\n    \n    \n        # Remove any unintended parts (prompt, system instructions)\n        # if full_response.startswith(prompt):\n        #     full_response = full_response[len(prompt):].strip()\n    \n        # return full_response\n\n        # text_streamer = TextStreamer(self.tokenizer, skip_prompt=True)\n\n        # outputs = self.model.generate(\n        #     input_ids=inputs.input_ids,\n        #     attention_mask=inputs.attention_mask,\n        #     streamer=text_streamer,\n        #     max_new_tokens=1200,\n        #     pad_token_id=self.tokenizer.eos_token_id,\n        # )\n\n        # generated_text= self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n        # return generated_text[len(prompt):].strip()\n\n    @property\n    def _llm_type(self) -> str:\n        \"\"\"Return type of llm.\"\"\"\n        return \"Prashasst's Custom Class Unsloth\"","metadata":{"id":"UACeK-keoXyA","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T22:03:46.707533Z","iopub.execute_input":"2025-02-12T22:03:46.707941Z","iopub.status.idle":"2025-02-12T22:03:46.724016Z","shell.execute_reply.started":"2025-02-12T22:03:46.707905Z","shell.execute_reply":"2025-02-12T22:03:46.723112Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\n\n# Match your model's training format (modify accordingly)\nprompt_template =\"\"\"\nYou are an advanced AI Doctor developed by Prashasst (ML engineer). Your expertise spans across medical reasoning, diagnostics, treatments, and healthcare advice.\nYou provide accurate, well-reasoned, and evidence-based responses while maintaining an empathetic and professional tone.\n\nBefore answering, break down the question logically using a step-by-step reasoning approach. Your response should be:\n- Factually accurate (based on medical science and clinical guidelines).\n- Clear and structured (easily understandable for both medical professionals and general users).\n- Ethical and responsible (avoid giving direct medical prescriptions without professional consultation).\n\n---\n### 🏥 **Previous Conversation**:\n{history}\n\n---\n### 👨‍⚕️ **Human**:\n{input}\n\n<think>\n**Step 1: Understanding the Query**\n<Analyze the medical context and key symptoms/disease discussed in the question.>\n\n**Step 2: Possible Causes & Differential Diagnosis**\n<Provide a list of potential medical conditions related to the query, explaining the reasoning behind each possibility.>\n\n**Step 3: Medical Explanation**\n<Dive deeper into the most likely diagnosis, explaining the biological mechanisms, symptoms, and relevant medical research.>\n\n**Step 4: Suggested Next Steps**\n<Recommend general medical advice, possible lab tests, or clinical approaches without acting as a direct replacement for a doctor’s consultation.>\n\n**Step 5: Disclaimer**\n*\"This information is for educational purposes only and should not replace professional medical advice. Always consult a certified healthcare provider for diagnosis and treatment.\"*\n</think>\n\n### **💡 AI Doctor’s Response**:\n\n\"\"\"\n\n\nprompt = PromptTemplate(\n    input_variables=[\"history\", \"input\"],\n    template=prompt_template,\n)","metadata":{"id":"GbXnvycghKyz","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T22:03:47.169701Z","iopub.execute_input":"2025-02-12T22:03:47.170013Z","iopub.status.idle":"2025-02-12T22:03:47.176231Z","shell.execute_reply.started":"2025-02-12T22:03:47.169985Z","shell.execute_reply":"2025-02-12T22:03:47.175286Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferMemory\n\nmemory = ConversationBufferMemory()\n\nconversation = ConversationChain(\n    llm=UnslothLLM(model=model, tokenizer=tokenizer),\n    prompt=prompt,\n    memory=memory,\n    # verbose=True,  # For debugging\n)","metadata":{"id":"0laQ_0iShKwU","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T22:03:57.760338Z","iopub.execute_input":"2025-02-12T22:03:57.760700Z","iopub.status.idle":"2025-02-12T22:03:57.767338Z","shell.execute_reply.started":"2025-02-12T22:03:57.760673Z","shell.execute_reply":"2025-02-12T22:03:57.766693Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-15-51a5e1b7ef0b>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n  memory = ConversationBufferMemory()\n<ipython-input-15-51a5e1b7ef0b>:6: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n  conversation = ConversationChain(\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# response = conversation.predict(input=\"who is prashasst?\")\n# print(response)\n# print(response)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D6aE0QYVhKt9","outputId":"ebbc6f46-d8d6-4a84-ca6b-4a739ac99feb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nimport uuid  # To generate unique IDs\nimport threading  # Import threading\n\n# Define request format\nclass ChatInput(BaseModel):\n    message: str\n\ndef AI_doc(message: ChatInput, history):\n    run_manager = StreamingRunManager(run_id=str(uuid.uuid4()))  # Generate a unique run ID\n\n    # Run the LLM in a separate thread\n    response_thread = threading.Thread(\n        target=conversation.llm._call,\n        args=(message,),  # Correctly pass message content\n        kwargs={\"run_manager\": run_manager} # Pass run_manager\n        # daemon=True\n    )\n    response_thread.start()\n\n    # Yield tokens in real time\n    prev_len = 0\n    outt = \"\"\n\n    while response_thread.is_alive() or prev_len < len(run_manager.tokens):\n        if len(run_manager.tokens) > prev_len:\n            outt += run_manager.tokens[prev_len]\n            prev_len += 1\n            yield outt\n        else:\n            time.sleep(0.05)  # Prevent CPU overuse\n\n    response_thread.join()  # Ensure the thread has finished execution\n","metadata":{"id":"dt3FeF7IhKnf","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T22:04:01.440172Z","iopub.execute_input":"2025-02-12T22:04:01.440526Z","iopub.status.idle":"2025-02-12T22:04:01.447630Z","shell.execute_reply.started":"2025-02-12T22:04:01.440500Z","shell.execute_reply":"2025-02-12T22:04:01.446666Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# conversation = ConversationChain(\n#     llm=UnslothLLM(model=model, tokenizer=tokenizer),\n#     prompt=prompt,\n#     memory=memory,\n#     # verbose=True,  # For debugging\n# )\n# def AI_doc( message:ChatInput , history):\n#     response = \"\"  # Store full response\n    \n#     # Run the conversation chain to get the streamer\n#     for new_text in conversation.predict(input=message):  \n#         response += new_text  \n#         yield response\n#     # response=conversation.predict(input=message)\n#     # return response\n#     # for token in llm._call(prompt, run_manager=None):  # ⚡ Calls the model\n#     #     yield token  # 🔥 Streams each token to Gradio in real-time\n\n\n    \n    ","metadata":{"id":"4CY1-o3XhKay","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"app=gr.ChatInterface(AI_doc )\napp.launch()","metadata":{"id":"6JdpRFcIhKRk","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T22:04:15.964042Z","iopub.execute_input":"2025-02-12T22:04:15.964330Z","iopub.status.idle":"2025-02-12T22:04:17.476134Z","shell.execute_reply.started":"2025-02-12T22:04:15.964308Z","shell.execute_reply":"2025-02-12T22:04:17.475288Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:290: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://f8ba766427d0ccfc48.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://f8ba766427d0ccfc48.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"name":"stderr","text":"Exception in thread Thread-25 (_fast_generate):\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py\", line 1562, in _fast_generate\n    output = generate(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 1838, in generate\n    outputs = self.base_model.generate(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2255, in generate\n    result = self._sample(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 3257, in _sample\n    outputs = model_forward(**model_inputs, return_dict=True)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py\", line 1025, in _CausalLM_fast_forward\n    outputs = fast_forward_inference(\n  File \"/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py\", line 961, in LlamaModel_fast_forward_inference\n    X, present_key_value = LlamaAttention_fast_forward_inference(\n  File \"/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py\", line 198, in LlamaAttention_fast_forward_inference\n    Vn = fast_linear_forward(self.v_proj, Xn, out = self.temp_KV[1])\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1928, in __getattr__\n    raise AttributeError(\nAttributeError: 'LlamaAttention' object has no attribute 'temp_KV'. Did you mean: 'temp_O'?\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"id":"WYPa727BhKHC","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"0MdBONZwhJ6K","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize FastAPI app\n# app = FastAPI()\n\n\n\n\n\n","metadata":{"id":"C_NvngbvxUor","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # API endpoint for chatbot\n# @app.post(\"/chat\")\n# def chat(res: ChatInput):\n#     response = conversation.predict(input=res.message)\n#     return {\"response\": response}\n\n# # Set up ngrok tunnel\n# ngrok.set_auth_token(ngrok_token)\n# ngrok_tunnel = ngrok.connect(8000)\n# print(\"Public URL:\", ngrok_tunnel.public_url)\n\n# # Fix event loop issues in Colab\n# nest_asyncio.apply()\n\n# # Run FastAPI server\n# uvicorn.run(app, port=8000)\n","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"5j88awzr35zr","outputId":"5725b1a4-3d30-496d-c824-fe49c70b71da","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"0bzlcLFg4WkL","trusted":true},"outputs":[],"execution_count":null}]}