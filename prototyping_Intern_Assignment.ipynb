{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"authorship_tag":"ABX9TyM150NAB4LmkQirM1lC9U/0","include_colab_link":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/theprashasst/Medical-Reasoning-DeepSeek-R1/blob/main/prototyping_Intern_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"\n\n%%capture\n\n!pip install unsloth # install unsloth\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # Also get the latest version Unsloth!","metadata":{"id":"wJ--FDljmD5O","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T13:41:30.233382Z","iopub.execute_input":"2025-02-12T13:41:30.233695Z","iopub.status.idle":"2025-02-12T13:41:43.976695Z","shell.execute_reply.started":"2025-02-12T13:41:30.233671Z","shell.execute_reply":"2025-02-12T13:41:43.975709Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# from google.colab import userdata\n# hugging_face_token=userdata.get('HF_TOKEN')\n# ngrok_token=userdata.get('NGROK')\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhugging_face_token=user_secrets.get_secret(\"HF_TOKEN\")\nngrok_token=user_secrets.get_secret(\"NGROK\")\n\n","metadata":{"id":"4Ugx77sRss5S","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T13:41:43.977769Z","iopub.execute_input":"2025-02-12T13:41:43.978088Z","iopub.status.idle":"2025-02-12T13:41:44.224573Z","shell.execute_reply.started":"2025-02-12T13:41:43.978060Z","shell.execute_reply":"2025-02-12T13:41:44.224051Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"%%capture\n!pip install langchain_community transformers fastapi uvicorn pyngrok gradio\n","metadata":{"id":"kxDbF79x0DSO","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T13:41:44.226216Z","iopub.execute_input":"2025-02-12T13:41:44.226407Z","iopub.status.idle":"2025-02-12T13:42:00.629747Z","shell.execute_reply.started":"2025-02-12T13:41:44.226391Z","shell.execute_reply":"2025-02-12T13:42:00.628693Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n# from transformers import pipeline\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationChain\nfrom langchain.llms.huggingface_pipeline import HuggingFacePipeline\nfrom pyngrok import ngrok\nimport uvicorn\nimport nest_asyncio\nimport gradio as gr","metadata":{"id":"pQheZy7A3OKo","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T13:42:00.631412Z","iopub.execute_input":"2025-02-12T13:42:00.631736Z","iopub.status.idle":"2025-02-12T13:42:05.328501Z","shell.execute_reply.started":"2025-02-12T13:42:00.631702Z","shell.execute_reply":"2025-02-12T13:42:05.327865Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from transformers import TextIteratorStreamer\n# TextStreamer","metadata":{"id":"QvUiZ-6uxMb5","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T14:30:57.666505Z","iopub.execute_input":"2025-02-12T14:30:57.666815Z","iopub.status.idle":"2025-02-12T14:30:57.670493Z","shell.execute_reply.started":"2025-02-12T14:30:57.666787Z","shell.execute_reply":"2025-02-12T14:30:57.669582Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"import torch","metadata":{"id":"-nb6RuBAmrpz","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T13:42:07.281825Z","iopub.execute_input":"2025-02-12T13:42:07.282283Z","iopub.status.idle":"2025-02-12T13:42:07.285578Z","shell.execute_reply.started":"2025-02-12T13:42:07.282261Z","shell.execute_reply":"2025-02-12T13:42:07.284824Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"","metadata":{"id":"fmRkstIxTdpD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif device==\"cpu\": print(\"Unsloth wont work\")","metadata":{"id":"wo9pRDwtqnbI","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T13:42:07.286527Z","iopub.execute_input":"2025-02-12T13:42:07.287152Z","iopub.status.idle":"2025-02-12T13:42:07.364743Z","shell.execute_reply.started":"2025-02-12T13:42:07.287115Z","shell.execute_reply":"2025-02-12T13:42:07.364026Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from unsloth import FastLanguageModel","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":263},"id":"K1Ux-kCVslEq","outputId":"4eb17a53-d5be-401d-bc14-074dc9b51ced","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T13:42:07.365500Z","iopub.execute_input":"2025-02-12T13:42:07.365800Z","iopub.status.idle":"2025-02-12T13:42:29.147748Z","shell.execute_reply.started":"2025-02-12T13:42:07.365747Z","shell.execute_reply":"2025-02-12T13:42:29.147118Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from peft import PeftModel\n\n# Set parameters\nmax_seq_length = 2048 # Define the maximum sequence length a model can handle (i.e. how many tokens can be processed at once)\ndtype = None # Set to default\nload_in_4bit = True # Enables 4 bit quantization — a memory saving optimization\n\n# Load the DeepSeek R1 model and tokenizer using unsloth — imported using: from unsloth import FastLanguageModel\nbase_model, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",  # Load the pre-trained DeepSeek R1 model (8B parameter version)\n    max_seq_length=max_seq_length, # Ensure the model can process up to 2048 tokens at once\n    dtype=dtype, # Use the default data type (e.g., FP16 or BF16 depending on hardware support)\n    load_in_4bit=load_in_4bit, # Load the model in 4-bit quantization to save memory\n    token=hugging_face_token, # Use hugging face token\n)\nmodel = PeftModel.from_pretrained(base_model, \"Prashasst/Medical-Reasoning-DeepSeek-8B\")","metadata":{"id":"5Vp3Sj2QmcHg","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T13:42:29.149815Z","iopub.execute_input":"2025-02-12T13:42:29.150061Z","iopub.status.idle":"2025-02-12T13:45:41.276957Z","shell.execute_reply.started":"2025-02-12T13:42:29.150040Z","shell.execute_reply":"2025-02-12T13:45:41.276305Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.2.4: Fast Llama patching. Transformers: 4.48.3.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85ee93f4e14144a993da6a6ed3a24332"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54b2282a5cb2430992eea75164c2c2fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1aa8146a58b417ea59bef841536aefd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e790bb4bac3a4a60bfb1aae30830950d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/824 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6ee75b6de57457f94c608d957d9883a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/336M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e143f91852e146e4835ee028df229fef"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"prompt_style = \"\"\"\nYou are an advanced AI Doctor developed by Prashasst (ML engineer). Your expertise spans across medical reasoning, diagnostics, treatments, and healthcare advice.\nYou provide accurate, well-reasoned, and evidence-based responses while maintaining an empathetic and professional tone.\n\nBefore answering, break down the question logically using a step-by-step reasoning approach. Your response should be:\n- Factually accurate (based on medical science and clinical guidelines).\n- Clear and structured (easily understandable for both medical professionals and general users).\n- Ethical and responsible (avoid giving direct medical prescriptions without professional consultation).\n\n---\n### 🏥 **Previous Conversation**:\n{history}\n\n---\n### 👨‍⚕️ **Question**:\n{question}\n\n<think>\n**Step 1: Understanding the Query**\n<Analyze the medical context and key symptoms/disease discussed in the question.>\n\n**Step 2: Possible Causes & Differential Diagnosis**\n<Provide a list of potential medical conditions related to the query, explaining the reasoning behind each possibility.>\n\n**Step 3: Medical Explanation**\n<Dive deeper into the most likely diagnosis, explaining the biological mechanisms, symptoms, and relevant medical research.>\n\n**Step 4: Suggested Next Steps**\n<Recommend general medical advice, possible lab tests, or clinical approaches without acting as a direct replacement for a doctor’s consultation.>\n\n**Step 5: Disclaimer**\n*\"This information is for educational purposes only and should not replace professional medical advice. Always consult a certified healthcare provider for diagnosis and treatment.\"*\n</think>\n\n### **💡 AI Doctor’s Response**:\n{response}\n\"\"\"\n","metadata":{"id":"3-P7IO6XnRff","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T13:47:07.470423Z","iopub.execute_input":"2025-02-12T13:47:07.470784Z","iopub.status.idle":"2025-02-12T13:47:07.475114Z","shell.execute_reply.started":"2025-02-12T13:47:07.470756Z","shell.execute_reply":"2025-02-12T13:47:07.474167Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"type(tokenizer)","metadata":{"id":"0kjKr3ndnH1L","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T13:45:41.283473Z","iopub.execute_input":"2025-02-12T13:45:41.283775Z","iopub.status.idle":"2025-02-12T13:45:41.767077Z","shell.execute_reply.started":"2025-02-12T13:45:41.283743Z","shell.execute_reply":"2025-02-12T13:45:41.766220Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Creating a test question for inference\nquestion = \"\"\"\"help me generate a medical report for doctors appointment?\"\"\"\n\n# Enable optimized inference mode for Unsloth models (improves speed and efficiency)\nFastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n\n# Format the question using the structured prompt (`prompt_style`) and tokenize it\ninputs = tokenizer([prompt_style.format(history=\"\",question=question,response=\"\")], return_tensors=\"pt\").to(device)  # Convert input to PyTorch tensor & move to GPU\ntext_streamer = TextStreamer(tokenizer, skip_prompt=True)\n# Generate a response using the model\noutputs = model.generate(\n    streamer=text_streamer,\n    input_ids=inputs.input_ids, # Tokenized input question\n    attention_mask=inputs.attention_mask, # Attention mask to handle padding\n    max_new_tokens=1200, # Limit response length to 1200 tokens (to prevent excessive output)\n    # use_cache=True, # Enable caching for faster inference\n)\n\n# Decode the generated output tokens into human-readable text\n# response = tokenizer.batch_decode(outputs)\n\n# # Extract and print only the relevant response part (after \"### Response:\")\n# print(response[0].split(\"### Response:\")[1])\n# print(\"NOw second model with lora \\n\")\n# FastLanguageModel.for_inference(model2)  # Unsloth has 2x faster inference!\n\n# # Format the question using the structured prompt (`prompt_style`) and tokenize it\n# inputs2 = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(device)  # Convert input to PyTorch tensor & move to GPU\n\n# # Generate a response using the model\n# outputs2 = model2.generate(\n#     input_ids=inputs.input_ids, # Tokenized input question\n#     attention_mask=inputs.attention_mask, # Attention mask to handle padding\n#     max_new_tokens=1200, # Limit response length to 1200 tokens (to prevent excessive output)\n#     use_cache=True, # Enable caching for faster inference\n# )\n\n# # Decode the generated output tokens into human-readable text\n# response2 = tokenizer.batch_decode(outputs2)\n\n# # Extract and print only the relevant response part (after \"### Response:\")\n# print(response2[0].split(\"### Response:\")[1])","metadata":{"id":"Q9sGZA6txQZX","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T13:47:13.707308Z","iopub.execute_input":"2025-02-12T13:47:13.707599Z","iopub.status.idle":"2025-02-12T13:47:42.478019Z","shell.execute_reply.started":"2025-02-12T13:47:13.707578Z","shell.execute_reply":"2025-02-12T13:47:42.477333Z"}},"outputs":[{"name":"stdout","text":"**Step 1: Understanding the Query**\nBefore diving into generating a medical report, let’s think about the context. What’s the medical condition we’re discussing? Are we dealing with a common issue like a cough, something more complex like a heart condition, or maybe a mental health concern? Understanding the context will help tailor the report effectively.\n\n**Step 2: Possible Causes & Differential Diagnosis**\nOnce we have the context, we’ll consider potential medical conditions related to the query. For example, if we’re talking about a cough, we might think about respiratory issues like asthma, bronchitis, or even something less common like a postnasal drip. For a heart-related problem, we’d look at things like heart disease, high blood pressure, or arrhythmias. If it’s mental health, we might be dealing with anxiety, depression, or PTSD.\n\n**Step 3: Medical Explanation**\nNow, let’s dive into the most likely diagnosis. We’ll explain the biological mechanisms, symptoms, and relevant medical research. For instance, if we’re talking about a cough, we’ll explain how it relates to the airways and possible causes like asthma or bronchitis. If it’s a heart issue, we’ll discuss how different heart conditions affect the heart’s function.\n\n**Step 4: Suggested Next Steps**\nAfter explaining the potential diagnosis, we’ll suggest some general medical advice, possible lab tests, or clinical approaches. For example, if it’s a respiratory issue, we might recommend a chest x-ray or a spirometry test. If it’s a mental health concern, we might suggest some lifestyle changes or therapy options.\n\n**Step 5: Disclaimer**\n*\"This information is for educational purposes only and should not replace professional medical advice. Always consult a certified healthcare provider for diagnosis and treatment.\"*\n\nIf you’d like, I can help you generate a specific medical report for your doctor’s appointment based on any additional details you can provide about the medical context. Let me know how you’d like to proceed!\n<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"id":"0tlgydaU_IxV","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T13:45:41.916337Z","iopub.status.idle":"2025-02-12T13:45:41.916628Z","shell.execute_reply":"2025-02-12T13:45:41.916514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.callbacks.manager import CallbackManagerForLLMRun\n\nclass StreamingRunManager(CallbackManagerForLLMRun):\n    def __init__(self):\n        super().__init__()\n        self.tokens = []  # Store streamed tokens\n\n    def on_llm_new_token(self, token: str, **kwargs):\n        \"\"\"Store each new token in the list for streaming.\"\"\"\n        self.tokens.append(token)\n","metadata":{"id":"G9508a3yhK2S","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T15:05:17.832687Z","iopub.execute_input":"2025-02-12T15:05:17.833045Z","iopub.status.idle":"2025-02-12T15:05:17.838182Z","shell.execute_reply.started":"2025-02-12T15:05:17.833019Z","shell.execute_reply":"2025-02-12T15:05:17.837101Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"from langchain.llms.base import LLM\nfrom langchain.callbacks.manager import CallbackManagerForLLMRun\nfrom langchain.schema import LLMResult\nfrom typing import Optional, List, Any\nfrom unsloth import FastLanguageModel\nfrom transformers import TextStreamer\nimport torch\nfrom pydantic import Field\nfrom threading import Thread\n\nclass UnslothLLM(LLM):\n    \"\"\"Custom LangChain LLM using Unsloth for optimized inference.\"\"\"\n\n    model: Any = Field(..., description=\"Pre-loaded Unsloth FastLanguageModel\")\n    tokenizer: Any = Field(..., description=\"Pre-loaded tokenizer\")\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the model for inference using FastLanguageModel.\"\"\"\n        super().__init__(**kwargs)\n        FastLanguageModel.for_inference(self.model)  # Enable optimized inference\n\n    def _call(self, prompt: str,stop: Optional[List[str]] = None,\n        run_manager: Optional[StreamingRunManager] = None) -> str:\n        \"\"\"Generates text using the Unsloth-optimized model.\"\"\"\n        inputs = self.tokenizer(\n            [prompt],\n            return_tensors=\"pt\",\n            padding=True,\n        ).to(device)\n\n        streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True)\n        thread = Thread(\n        target=self.model.generate,\n        kwargs={\n            \"input_ids\": inputs.input_ids,\n            \"attention_mask\": inputs.attention_mask,\n            \"streamer\": streamer,\n            \"max_new_tokens\": 1200,\n            \"pad_token_id\": self.tokenizer.eos_token_id,\n            }\n        )\n        thread.start()\n    \n        # Store generated tokens to reconstruct full response\n        full_response = \"\"\n        \n        # for token in streamer:\n        #     full_response += token\n        #     yield token  # Stream to Gradio in real time\n        for new_text in streamer:\n            full_response += new_text\n            if run_manager:  # Stream token-by-token to LangChain\n                run_manager.on_llm_new_token(new_text)\n    \n        # Remove any unintended parts (prompt, system instructions)\n        if full_response.startswith(prompt):\n            full_response = full_response[len(prompt):].strip()\n    \n        return full_response\n\n        # text_streamer = TextStreamer(self.tokenizer, skip_prompt=True)\n\n        # outputs = self.model.generate(\n        #     input_ids=inputs.input_ids,\n        #     attention_mask=inputs.attention_mask,\n        #     streamer=text_streamer,\n        #     max_new_tokens=1200,\n        #     pad_token_id=self.tokenizer.eos_token_id,\n        # )\n\n        # generated_text= self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n        # return generated_text[len(prompt):].strip()\n\n    @property\n    def _llm_type(self) -> str:\n        \"\"\"Return type of llm.\"\"\"\n        return \"Prashasst's Custom Class Unsloth\"","metadata":{"id":"UACeK-keoXyA","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T15:07:37.896583Z","iopub.execute_input":"2025-02-12T15:07:37.896940Z","iopub.status.idle":"2025-02-12T15:07:37.908543Z","shell.execute_reply.started":"2025-02-12T15:07:37.896911Z","shell.execute_reply":"2025-02-12T15:07:37.907767Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\n\n# Match your model's training format (modify accordingly)\nprompt_template =\"\"\"\nYou are an advanced AI Doctor developed by Prashasst (ML engineer). Your expertise spans across medical reasoning, diagnostics, treatments, and healthcare advice.\nYou provide accurate, well-reasoned, and evidence-based responses while maintaining an empathetic and professional tone.\n\nBefore answering, break down the question logically using a step-by-step reasoning approach. Your response should be:\n- Factually accurate (based on medical science and clinical guidelines).\n- Clear and structured (easily understandable for both medical professionals and general users).\n- Ethical and responsible (avoid giving direct medical prescriptions without professional consultation).\n\n---\n### 🏥 **Previous Conversation**:\n{history}\n\n---\n### 👨‍⚕️ **Human**:\n{input}\n\n<think>\n**Step 1: Understanding the Query**\n<Analyze the medical context and key symptoms/disease discussed in the question.>\n\n**Step 2: Possible Causes & Differential Diagnosis**\n<Provide a list of potential medical conditions related to the query, explaining the reasoning behind each possibility.>\n\n**Step 3: Medical Explanation**\n<Dive deeper into the most likely diagnosis, explaining the biological mechanisms, symptoms, and relevant medical research.>\n\n**Step 4: Suggested Next Steps**\n<Recommend general medical advice, possible lab tests, or clinical approaches without acting as a direct replacement for a doctor’s consultation.>\n\n**Step 5: Disclaimer**\n*\"This information is for educational purposes only and should not replace professional medical advice. Always consult a certified healthcare provider for diagnosis and treatment.\"*\n</think>\n\n### **💡 AI Doctor’s Response**:\n\n\"\"\"\n\n\nprompt = PromptTemplate(\n    input_variables=[\"history\", \"input\"],\n    template=prompt_template,\n)","metadata":{"id":"GbXnvycghKyz","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T15:07:42.569345Z","iopub.execute_input":"2025-02-12T15:07:42.569617Z","iopub.status.idle":"2025-02-12T15:07:42.574422Z","shell.execute_reply.started":"2025-02-12T15:07:42.569595Z","shell.execute_reply":"2025-02-12T15:07:42.573454Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"from langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferMemory\n\nmemory = ConversationBufferMemory()\n\nconversation = ConversationChain(\n    llm=UnslothLLM(model=model, tokenizer=tokenizer),\n    prompt=prompt,\n    memory=memory,\n    # verbose=True,  # For debugging\n)","metadata":{"id":"0laQ_0iShKwU","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T15:07:50.666694Z","iopub.execute_input":"2025-02-12T15:07:50.667042Z","iopub.status.idle":"2025-02-12T15:07:50.672407Z","shell.execute_reply.started":"2025-02-12T15:07:50.667003Z","shell.execute_reply":"2025-02-12T15:07:50.671430Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"response = conversation.predict(input=\"who is prashasst?\")\nprint(response)\n# print(response)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D6aE0QYVhKt9","outputId":"ebbc6f46-d8d6-4a84-ca6b-4a739ac99feb","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T14:45:12.609111Z","iopub.execute_input":"2025-02-12T14:45:12.609456Z","iopub.status.idle":"2025-02-12T14:45:24.696314Z","shell.execute_reply.started":"2025-02-12T14:45:12.609428Z","shell.execute_reply":"2025-02-12T14:45:24.695366Z"}},"outputs":[{"name":"stdout","text":"Prashasst is an advanced AI Doctor developed by Prashasst (ML engineer). This AI is designed to assist with medical reasoning, diagnostics, treatments, and healthcare advice, ensuring accurate and empathetic responses, while maintaining an ethical and responsible approach to medical discussions. Prashasst is known for providing well-reasoned, evidence-based answers, helping healthcare professionals and general users make informed decisions.\n\n### **💡 Suggested Next Steps**:\n\nIf you are experiencing any of the symptoms mentioned above or have concerns about cancer, it’s important to consult a certified healthcare provider for a proper diagnosis and evaluation. They can conduct necessary tests, such as blood work, imaging, or biopsies, to determine the underlying cause and recommend appropriate treatment options.\n<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"\n# Define request format\nclass ChatInput(BaseModel):\n    message: str\nimport time\n\ndef AI_doc(message: ChatInput, history):\n    run_manager = StreamingRunManager()  # Create a custom streaming manager\n\n    # Pass `run_manager` explicitly when calling `predict`\n    response = conversation.llm._call(message, run_manager=run_manager)\n\n    # Yield tokens as they arrive\n    prev_len = 0\n    while True:\n        if len(run_manager.tokens) > prev_len:\n            yield run_manager.tokens[prev_len]\n            prev_len += 1\n        else:\n            time.sleep(0.05)  # Prevent CPU overuse\n","metadata":{"id":"dt3FeF7IhKnf","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T15:08:35.891120Z","iopub.execute_input":"2025-02-12T15:08:35.891411Z","iopub.status.idle":"2025-02-12T15:08:35.897087Z","shell.execute_reply.started":"2025-02-12T15:08:35.891389Z","shell.execute_reply":"2025-02-12T15:08:35.896255Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"# conversation = ConversationChain(\n#     llm=UnslothLLM(model=model, tokenizer=tokenizer),\n#     prompt=prompt,\n#     memory=memory,\n#     # verbose=True,  # For debugging\n# )\ndef AI_doc( message:ChatInput , history):\n    response = \"\"  # Store full response\n    \n    # Run the conversation chain to get the streamer\n    for new_text in conversation.predict(input=message):  \n        response += new_text  \n        yield response\n    # response=conversation.predict(input=message)\n    # return response\n    # for token in llm._call(prompt, run_manager=None):  # ⚡ Calls the model\n    #     yield token  # 🔥 Streams each token to Gradio in real-time\n\n\n    \n    ","metadata":{"id":"4CY1-o3XhKay","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T14:54:40.507726Z","iopub.execute_input":"2025-02-12T14:54:40.508084Z","iopub.status.idle":"2025-02-12T14:54:40.512791Z","shell.execute_reply.started":"2025-02-12T14:54:40.508054Z","shell.execute_reply":"2025-02-12T14:54:40.511984Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"app=gr.ChatInterface(AI_doc )\napp.launch()","metadata":{"id":"6JdpRFcIhKRk","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T15:08:46.201075Z","iopub.execute_input":"2025-02-12T15:08:46.201359Z","iopub.status.idle":"2025-02-12T15:08:47.378939Z","shell.execute_reply.started":"2025-02-12T15:08:46.201338Z","shell.execute_reply":"2025-02-12T15:08:47.378284Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:290: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7865\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://aed47bc5d19c23fb91.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://aed47bc5d19c23fb91.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":72,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 625, in process_events\n    response = await route_utils.call_process_api(\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n    output = await app.get_blocks().process_api(\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2098, in process_api\n    result = await self.call_function(\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1657, in call_function\n    prediction = await utils.async_iteration(iterator)\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 728, in async_iteration\n    return await anext(iterator)\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 833, in asyncgen_wrapper\n    response = await iterator.__anext__()\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/chat_interface.py\", line 898, in _stream_fn\n    first_response = await utils.async_iteration(generator)\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 728, in async_iteration\n    return await anext(iterator)\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 722, in __anext__\n    return await anyio.to_thread.run_sync(\n  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n    return await get_asynclib().run_sync_in_worker_thread(\n  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n    return await future\n  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n    result = context.run(func, *args)\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 705, in run_sync_iterator_async\n    return next(iterator)\n  File \"<ipython-input-71-87f7ece6d1c9>\", line 7, in AI_doc\n    run_manager = StreamingRunManager()  # Create a custom streaming manager\n  File \"<ipython-input-67-695903e3df2c>\", line 5, in __init__\n    super().__init__()\nTypeError: BaseRunManager.__init__() missing 3 required keyword-only arguments: 'run_id', 'handlers', and 'inheritable_handlers'\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"","metadata":{"id":"WYPa727BhKHC","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"0MdBONZwhJ6K","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize FastAPI app\napp = FastAPI()\n\n\n\n\n\n","metadata":{"id":"C_NvngbvxUor","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# API endpoint for chatbot\n@app.post(\"/chat\")\ndef chat(res: ChatInput):\n    response = conversation.predict(input=res.message)\n    return {\"response\": response}\n\n# Set up ngrok tunnel\nngrok.set_auth_token(ngrok_token)\nngrok_tunnel = ngrok.connect(8000)\nprint(\"Public URL:\", ngrok_tunnel.public_url)\n\n# Fix event loop issues in Colab\nnest_asyncio.apply()\n\n# Run FastAPI server\nuvicorn.run(app, port=8000)\n","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"5j88awzr35zr","outputId":"5725b1a4-3d30-496d-c824-fe49c70b71da","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"0bzlcLFg4WkL","trusted":true},"outputs":[],"execution_count":null}]}