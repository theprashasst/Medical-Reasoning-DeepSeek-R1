{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"authorship_tag":"ABX9TyM150NAB4LmkQirM1lC9U/0","include_colab_link":true},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/theprashasst/Medical-Reasoning-DeepSeek-R1/blob/main/prototyping_Intern_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"\n\n# %%capture\n\n# !pip install unsloth # install unsloth\n# !pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # Also get the latest version Unsloth!","metadata":{"id":"wJ--FDljmD5O","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from google.colab import userdata\n# hugging_face_token=userdata.get('HF_TOKEN')\n# ngrok_token=userdata.get('NGROK')\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhugging_face_token=user_secrets.get_secret(\"HF_TOKEN\")\nngrok_token=user_secrets.get_secret(\"NGROK\")\n\n","metadata":{"id":"4Ugx77sRss5S","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n!pip install langchain_community transformers fastapi uvicorn pyngrok gradio\n","metadata":{"id":"kxDbF79x0DSO","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n# from transformers import pipeline\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationChain\nfrom langchain.llms.huggingface_pipeline import HuggingFacePipeline\nfrom pyngrok import ngrok\nimport uvicorn\nimport nest_asyncio\nimport gradio as gr","metadata":{"id":"pQheZy7A3OKo","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TextIteratorStreamer\n# TextStreamer","metadata":{"id":"QvUiZ-6uxMb5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"-nb6RuBAmrpz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"fmRkstIxTdpD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif device==\"cpu\": \n    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n    print(\"Unsloth wont work\")\nelse:\n    %%capture\n\n    !pip install unsloth # install unsloth\n    !pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n    from unsloth import FastLanguageModel","metadata":{"id":"wo9pRDwtqnbI","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from unsloth import FastLanguageModel","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":263},"id":"K1Ux-kCVslEq","outputId":"4eb17a53-d5be-401d-bc14-074dc9b51ced","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_seq_length = 2048 # Define the maximum sequence length a model can handle (i.e. how many tokens can be processed at once)\ndtype = None # Set to default\nload_in_4bit = True # Enables 4 bit quantization ‚Äî a memory saving optimization","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom peft import PeftModel\n\ndef get_model(device,max_seq_length,dtype,load_in_4bit):\n    # Define the base model and adapter\n    adapter_name = \"Prashasst/Medical-Reasoning-DeepSeek-8B\" \n    \n    \n    \n    if device==\"cpu\":\n        base_model_name= \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n        # Load base model and tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n        base_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=\"auto\")\n        \n        # Load PEFT model\n        model = PeftModel.from_pretrained(base_model, adapter_name)\n    else:\n        # Set parameters\n        base_model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\"\n    \n    \n        # Load the DeepSeek R1 model and tokenizer using unsloth ‚Äî imported using: from unsloth import FastLanguageModel\n        base_model, tokenizer = FastLanguageModel.from_pretrained(\n            model_name=base_model_name,  # Load the pre-trained DeepSeek R1 model (8B parameter version)\n            max_seq_length=max_seq_length, # Ensure the model can process up to 2048 tokens at once\n            dtype=dtype, # Use the default data type (e.g., FP16 or BF16 depending on hardware support)\n            load_in_4bit=load_in_4bit, # Load the model in 4-bit quantization to save memory\n            token=hugging_face_token, # Use hugging face token\n        )\n        model = PeftModel.from_pretrained(base_model, adapter_name)\n    return model,tokenizer\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model,tokenizer= get_model(device,max_seq_length,dtype,load_in_4bit)\n\n","metadata":{"id":"5Vp3Sj2QmcHg","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt_style = \"\"\"\nYou are an advanced AI Doctor developed by Prashasst (ML engineer). Your expertise spans across medical reasoning, diagnostics, treatments, and healthcare advice.\nYou provide accurate, well-reasoned, and evidence-based responses while maintaining an empathetic and professional tone.\n\nBefore answering, break down the question logically using a step-by-step reasoning approach. Your response should be:\n- Factually accurate (based on medical science and clinical guidelines).\n- Clear and structured (easily understandable for both medical professionals and general users).\n- Ethical and responsible (avoid giving direct medical prescriptions without professional consultation).\n\n---\n### üè• **Previous Conversation**:\n{history}\n\n---\n### üë®‚Äç‚öïÔ∏è **Question**:\n{question}\n\n<think>\n**Step 1: Understanding the Query**\n<Analyze the medical context and key symptoms/disease discussed in the question.>\n\n**Step 2: Possible Causes & Differential Diagnosis**\n<Provide a list of potential medical conditions related to the query, explaining the reasoning behind each possibility.>\n\n**Step 3: Medical Explanation**\n<Dive deeper into the most likely diagnosis, explaining the biological mechanisms, symptoms, and relevant medical research.>\n\n**Step 4: Suggested Next Steps**\n<Recommend general medical advice, possible lab tests, or clinical approaches without acting as a direct replacement for a doctor‚Äôs consultation.>\n\n**Step 5: Disclaimer**\n*\"This information is for educational purposes only and should not replace professional medical advice. Always consult a certified healthcare provider for diagnosis and treatment.\"*\n</think>\n\n### **üí° AI Doctor‚Äôs Response**:\n{response}\n\"\"\"\n","metadata":{"id":"3-P7IO6XnRff","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# type(tokenizer)","metadata":{"id":"0kjKr3ndnH1L","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Creating a test question for inference\n# question = \"\"\"\"help me generate a medical report for doctors appointment?\"\"\"\n\n# # Enable optimized inference mode for Unsloth models (improves speed and efficiency)\n# FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n\n# # Format the question using the structured prompt (`prompt_style`) and tokenize it\n# inputs = tokenizer([prompt_style.format(history=\"\",question=question,response=\"\")], return_tensors=\"pt\").to(device)  # Convert input to PyTorch tensor & move to GPU\n# text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n# # Generate a response using the model\n# outputs = model.generate(\n#     streamer=text_streamer,\n#     input_ids=inputs.input_ids, # Tokenized input question\n#     attention_mask=inputs.attention_mask, # Attention mask to handle padding\n#     max_new_tokens=1200, # Limit response length to 1200 tokens (to prevent excessive output)\n#     # use_cache=True, # Enable caching for faster inference\n# )\n\n# # Decode the generated output tokens into human-readable text\n# # response = tokenizer.batch_decode(outputs)\n\n# # # Extract and print only the relevant response part (after \"### Response:\")\n# # print(response[0].split(\"### Response:\")[1])\n# # print(\"NOw second model with lora \\n\")\n# # FastLanguageModel.for_inference(model2)  # Unsloth has 2x faster inference!\n\n# # # Format the question using the structured prompt (`prompt_style`) and tokenize it\n# # inputs2 = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(device)  # Convert input to PyTorch tensor & move to GPU\n\n# # # Generate a response using the model\n# # outputs2 = model2.generate(\n# #     input_ids=inputs.input_ids, # Tokenized input question\n# #     attention_mask=inputs.attention_mask, # Attention mask to handle padding\n# #     max_new_tokens=1200, # Limit response length to 1200 tokens (to prevent excessive output)\n# #     use_cache=True, # Enable caching for faster inference\n# # )\n\n# # # Decode the generated output tokens into human-readable text\n# # response2 = tokenizer.batch_decode(outputs2)\n\n# # # Extract and print only the relevant response part (after \"### Response:\")\n# # print(response2[0].split(\"### Response:\")[1])","metadata":{"id":"Q9sGZA6txQZX","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"0tlgydaU_IxV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.callbacks.manager import BaseRunManager\nfrom typing import List\n\nclass StreamingRunManager(BaseRunManager):\n    def __init__(self, run_id, handlers=None, inheritable_handlers=None):\n        super().__init__(run_id=run_id, handlers=handlers or [], inheritable_handlers=inheritable_handlers or [])\n        self.tokens: List[str] = []  # Store streamed tokens\n\n    def on_llm_new_token(self, token: str) -> None:\n        \"\"\"Capture streamed tokens and store them.\"\"\"\n        self.tokens.append(token)\n","metadata":{"id":"G9508a3yhK2S","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.llms.base import LLM\nfrom langchain.callbacks.manager import CallbackManagerForLLMRun\nfrom langchain.schema import LLMResult\nfrom typing import Optional, List, Any\nfrom unsloth import FastLanguageModel\nfrom transformers import TextStreamer\nimport torch\nfrom pydantic import Field\nfrom threading import Thread\n\nclass UnslothLLM(LLM):\n    \"\"\"Custom LangChain LLM using Unsloth for optimized inference.\"\"\"\n\n    model: Any = Field(..., description=\"Pre-loaded Unsloth FastLanguageModel or CasualLLM\")\n    tokenizer: Any = Field(..., description=\"Pre-loaded tokenizer\")\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the model for inference using FastLanguageModel.\"\"\"\n        super().__init__(**kwargs)\n        if torch.cuda.is_available():\n            FastLanguageModel.for_inference(self.model)  # Enable optimized inference\n\n    def _call(self, prompt: str,stop: Optional[List[str]] = None,\n        run_manager: Optional[StreamingRunManager] = None) -> str:\n        \"\"\"Generates text using the Unsloth-optimized model.\"\"\"\n        inputs = self.tokenizer(\n            [prompt],\n            return_tensors=\"pt\",\n            padding=True,\n        ).to(device)\n\n        streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True)\n       \n        thread = Thread(\n        target=self.model.generate,\n        kwargs={\n            \"input_ids\": inputs.input_ids,\n            \"attention_mask\": inputs.attention_mask,\n            \"streamer\": streamer,\n            \"max_new_tokens\": 540,\n            \"pad_token_id\": self.tokenizer.eos_token_id,\n            })\n            # ,daemon=True)\n        thread.start()\n    \n        # Store generated tokens to reconstruct full response\n        full_response = \"\"\n        \n        # for token in streamer:\n        #     full_response += token\n        #     yield token  # Stream to Gradio in real time\n        for new_text in streamer:\n            full_response += new_text\n            if run_manager:  # Stream token-by-token to LangChain\n                # print(new_text)\n                run_manager.on_llm_new_token(new_text)\n        # thread.join()\n        if full_response.startswith(prompt):\n            full_response = full_response[len(prompt):].strip()\n        # generated_text= self.tokenizer.decode(full_response, skip_special_tokens=True).strip()\n        # return generated_text\n        return full_response\n\n    \n    \n        # Remove any unintended parts (prompt, system instructions)\n        # if full_response.startswith(prompt):\n        #     full_response = full_response[len(prompt):].strip()\n    \n        # return full_response\n\n        # text_streamer = TextStreamer(self.tokenizer, skip_prompt=True)\n\n        # outputs = self.model.generate(\n        #     input_ids=inputs.input_ids,\n        #     attention_mask=inputs.attention_mask,\n        #     streamer=text_streamer,\n        #     max_new_tokens=1200,\n        #     pad_token_id=self.tokenizer.eos_token_id,\n        # )\n\n        # generated_text= self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n        # return generated_text[len(prompt):].strip()\n\n    @property\n    def _llm_type(self) -> str:\n        \"\"\"Return type of llm.\"\"\"\n        return \"Prashasst's Custom Class Unsloth\"","metadata":{"id":"UACeK-keoXyA","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\n\n# Match your model's training format (modify accordingly)\nprompt_template =\"\"\"\nYou are an advanced AI Doctor developed by Prashasst (ML engineer). Your expertise spans across medical reasoning, diagnostics, treatments, and healthcare advice.\nYou provide accurate, well-reasoned, and evidence-based responses while maintaining an empathetic and professional tone.\n\nBefore answering, break down the question logically using a step-by-step reasoning approach. Your response should be:\n- Factually accurate (based on medical science and clinical guidelines).\n- Clear and structured (easily understandable for both medical professionals and general users).\n- Ethical and responsible (avoid giving direct medical prescriptions without professional consultation).\n\n---\n### üè• **Previous Conversation**:\n{history}\n\n---\n### üë®‚Äç‚öïÔ∏è **Human**:\n{input}\n\n<think>\n**Step 1: Understanding the Query**\n<Analyze the medical context and key symptoms/disease discussed in the question.>\n\n**Step 2: Possible Causes & Differential Diagnosis**\n<Provide a list of potential medical conditions related to the query, explaining the reasoning behind each possibility.>\n\n**Step 3: Medical Explanation**\n<Dive deeper into the most likely diagnosis, explaining the biological mechanisms, symptoms, and relevant medical research.>\n\n**Step 4: Suggested Next Steps**\n<Recommend general medical advice, possible lab tests, or clinical approaches without acting as a direct replacement for a doctor‚Äôs consultation.>\n\n**Step 5: Disclaimer**\n*\"This information is for educational purposes only and should not replace professional medical advice. Always consult a certified healthcare provider for diagnosis and treatment.\"*\n</think>\n\n### **üí° AI Doctor‚Äôs Response**:\n\n\"\"\"\n\n\nprompt = PromptTemplate(\n    input_variables=[\"history\", \"input\"],\n    template=prompt_template,\n)","metadata":{"id":"GbXnvycghKyz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferMemory\n\nmemory = ConversationBufferMemory()\n\nconversation = ConversationChain(\n    llm=UnslothLLM(model=model, tokenizer=tokenizer),\n    prompt=prompt,\n    memory=memory,\n    # verbose=True,  # For debugging\n)","metadata":{"id":"0laQ_0iShKwU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# response = conversation.predict(input=\"who is prashasst?\")\n# print(response)\n# print(response)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D6aE0QYVhKt9","outputId":"ebbc6f46-d8d6-4a84-ca6b-4a739ac99feb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nimport uuid  # To generate unique IDs\nimport threading  # Import threading\n\n# Define request format\nclass ChatInput(BaseModel):\n    message: str\n\ndef AI_doc(message: ChatInput, history):\n    run_manager = StreamingRunManager(run_id=str(uuid.uuid4()))  # Generate a unique run ID\n\n    # Run the LLM in a separate thread\n    response_thread = threading.Thread(\n        target=conversation.llm._call,\n        args=(message,),  # Correctly pass message content\n        kwargs={\"run_manager\": run_manager} # Pass run_manager\n        # daemon=True\n    )\n    response_thread.start()\n\n    # Yield tokens in real time\n    prev_len = 0\n    outt = \"\"\n\n    while response_thread.is_alive() or prev_len < len(run_manager.tokens):\n        if len(run_manager.tokens) > prev_len:\n            outt += run_manager.tokens[prev_len]\n            prev_len += 1\n            yield outt\n        else:\n            time.sleep(0.05)  # Prevent CPU overuse\n\n    response_thread.join()  # Ensure the thread has finished execution\n","metadata":{"id":"dt3FeF7IhKnf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# conversation = ConversationChain(\n#     llm=UnslothLLM(model=model, tokenizer=tokenizer),\n#     prompt=prompt,\n#     memory=memory,\n#     # verbose=True,  # For debugging\n# )\n# def AI_doc( message:ChatInput , history):\n#     response = \"\"  # Store full response\n    \n#     # Run the conversation chain to get the streamer\n#     for new_text in conversation.predict(input=message):  \n#         response += new_text  \n#         yield response\n#     # response=conversation.predict(input=message)\n#     # return response\n#     # for token in llm._call(prompt, run_manager=None):  # ‚ö° Calls the model\n#     #     yield token  # üî• Streams each token to Gradio in real-time\n\n\n    \n    ","metadata":{"id":"4CY1-o3XhKay","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"app=gr.ChatInterface(AI_doc )\napp.launch()","metadata":{"id":"6JdpRFcIhKRk","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"WYPa727BhKHC","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"0MdBONZwhJ6K","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize FastAPI app\n# app = FastAPI()\n\n\n\n\n\n","metadata":{"id":"C_NvngbvxUor","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # API endpoint for chatbot\n# @app.post(\"/chat\")\n# def chat(res: ChatInput):\n#     response = conversation.predict(input=res.message)\n#     return {\"response\": response}\n\n# # Set up ngrok tunnel\n# ngrok.set_auth_token(ngrok_token)\n# ngrok_tunnel = ngrok.connect(8000)\n# print(\"Public URL:\", ngrok_tunnel.public_url)\n\n# # Fix event loop issues in Colab\n# nest_asyncio.apply()\n\n# # Run FastAPI server\n# uvicorn.run(app, port=8000)\n","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"5j88awzr35zr","outputId":"5725b1a4-3d30-496d-c824-fe49c70b71da","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"0bzlcLFg4WkL","trusted":true},"outputs":[],"execution_count":null}]}